---
title: "Area of Interest Report - Oculina Bank HAPC"
author: "Robert P. McGuinn"
date: "November 27, 2024"
output: word_document
---


```{r packages, eval=T, echo=F, message=FALSE, warning=FALSE, cache=FALSE}
## load packages
#install.packages(tidyverse)
#install.packages('openxlsx')
library(tidyverse)
library(openxlsx)
library(sf)
library(RColorBrewer)
library(raster)
library(leaflet)
library(extrafont)
library(RColorBrewer)
library(rmarkdown)
library(knitr)
library(maps)
library(raster)
library(httr)
library(jsonlite)
library(rnaturalearth)
library(rnaturalearthdata)
library(openxlsx)
library(googledrive)


```

```{r load_NDB, eval=F, echo=F, message=FALSE, warning=FALSE, cache=TRUE}
## load NDB
source('c:/rworking/deepseatools/code/mod_load_current_NDB.R')

```

```{r, set_version, echo=F}
## create needed variables
db_version <- unique(filt$DatabaseVersion)

```

```{r filter_geo_OR_query, echo=F, warning=F, message=F, eval=T}
##### filter occurrences by query or box or both ##### 

## set bounding box variables
minlat <- 25
maxlat <- 35
minlon <- -82
maxlon <- -71

## subset data by coordinates
x <- subset(filt, as.numeric(Latitude) > minlat &
                   as.numeric(Latitude) < maxlat &
                   as.numeric(Longitude) > minlon &
                   as.numeric(Longitude) < maxlon)
                
## also subset data by other variables by other criteria
# x <- x %>% filter(ScientificName == "")

## checking
# dim(x)

```

```{r create_geo_from_points, echo=FALSE, echo = FALSE, message = FALSE, warning=FALSE, eval=T}
## create spatial points data frame from a subset of points
x_geo <- x

## create spdf
coordinates(x_geo) <- c("Longitude", "Latitude")
proj4string(x_geo) <- "+proj=longlat +ellps=WGS84 +datum=WGS84"

## OPTIONAL: write out featureclass to geodatabase from sp object 
# fgdb_path <- "C:/data/aprx/aoi/aoi.gdb"
# arc.write(file.path(fgdb_path, 'x_geo'), data=x_geo, overwrite = TRUE)

```

```{r create_sf_from_points, echo=F}
## transform coral and sponge occurrence points to 'sf' objects
##### transform coral and sponge points to sf #####
points <- st_as_sf(x_geo, wkt = "geom")

## OPTIONAL: write out featureclass from sf object
# fgdb_path <- "C:/data/aprx/aoi/aoi.gdb"
# arc.write(file.path(fgdb_path, 'points'), data=points, overwrite = TRUE)
rm(x)

```

```{r import_aoi, echo=FALSE, echo = F, message = FALSE, warning=FALSE, eval=T}
shapefile_path <- "C:/rworking/deepseatools/indata/20221104_protected_areas_update_HColeman/20221104_protected_areas.shp"
protected_areas <- st_read(shapefile_path)

##### get the Oculina HAPC ##### 
oculina <- protected_areas %>% filter(Sitename == 'Oculina Bank HAPC')
```

```{r buffering, echo=F}
##### buffer operations ##### 
## create 5000 meter buffer
oculina_buf_5000 <- st_buffer(oculina, dist=5000) # map units are in meters

##create 20000 meter buffer
oculina_buf_20000 <- st_buffer(oculina, dist=20000) # map units are in meters


```

```{r intersect, echo=F}
##### intersection operations ##### 
## transform CRS for coral and sponge points projection to match polygons
points_transform <- st_transform(points, crs = st_crs(oculina))

## checking
# st_crs(points_transform)

## plot to see how they relate
# bbox <- st_bbox(oculina_buf_20000)
# 
# ggplot() +
#   geom_sf(data = points_transform, color = "blue", size = .2) +
#   geom_sf(data = oculina_buf_20000, fill = "red", alpha = 0.3) +
#   geom_sf(data = oculina_buf_5000, fill = "black", alpha = 0.3) +
#   geom_sf(data = oculina, fill = "green", alpha = 0.3) +
#   theme_minimal() +
#   coord_sf(
#     xlim = c(bbox["xmin"], bbox["xmax"]),
#     ylim = c(bbox["ymin"], bbox["ymax"])
#   ) +
#   labs(
#     title = "Oculina Banks HAPC with 20km buffer",
#     subtitle = "Coral and sponge occurences in the buffer area",
#     caption = paste("Data source: NOAA National Database for Deep-sea Corals and Sponges: ", db_version, sep = '')
#   )


## find points within the polygons
points_in_0 <- st_join(points_transform, oculina, join = st_within) # no buffer
points_in_5000 <- st_join(points_transform, oculina_buf_5000, join = st_within) # 5000 meter buffer
points_in_20000 <- st_join(points_transform, oculina_buf_20000, join = st_within) # 20000 meter buffer

## checking
# dim(points_in_0)
# dim(points_in_5000)
# dim(points_in_20000)
# table(points_in_0$Shape_Area, useNA = 'always')
# table(points_in_5000$Shape_Area, useNA = 'always')
# table(points_in_20000$Shape_Area, useNA = 'always')

## select intersecting points only and create separate data frames
points_in_0_select <- points_in_0 %>% filter(is.na(Shape_Area) == F)
points_in_5000_select <- points_in_5000 %>% filter(is.na(Shape_Area) == F)
points_in_20000_select <- points_in_20000 %>% filter(is.na(Shape_Area) == F)

## checking 
# dim(points_in_0_select)
# dim(points_in_5000_select)
# dim(points_in_20000_select)


```

```{r load_images_to_folder, eval=F, echo=F}
## load any images to a local folder 
#### WARNING: This chunk takes a long time.  Go get a coffee #####
###### download all images and put them in a folder #####
z <- points_in_0_select %>% filter(is.na(ImageURL) == F)

## check 
length(z$ImageFilePath)

dir.create('C:/rworking/deepseatools/indata/imageset_0', recursive = TRUE)
setwd("C:/rworking/deepseatools/indata/imageset_0")

for(i in 1:length(z$CatalogNumber)){
  download.file(as.character(z$ImageURL[i]),
                destfile = paste("DSCRTP",
                                 z$CatalogNumber[i],
                                 z$ScientificName[i],
                                 z$DepthInMeters[i],
                                 basename(as.character(z$ImageURL[i])),
                                 sep = '_'),
                mode = "wb")
}

```

```{r eval=F, echo=F}
# load the images to a Google Drive folder
## WARNING: This chunk could take a longish time. Go get a coffee

##### Loading files from a local folder to Google Drive ##### 
## MANUAL CHANGE "folderurl" to the desired drive folder ID
folderurl <- "https://drive.google.com/drive/folders/1oOiCVRWJUc4fQQ1dec9q3APYfk1Ea4nj"

## get the list of files from the local folder
files <- list.files(path="C:/rworking/deepseatools/indata/imageset_5000", full.names=TRUE, recursive=FALSE)

## loop upload images to Google Drive
for(i in files){
drive_upload(i,
             path = as_id(folderurl),
             overwrite = T)
}

```

```{r echo=F}
z <- points_in_5000_select %>% filter(is.na(ImageURL) == F)
```

```{r cache=T, echo=F}
## get OBIS data 
## first get the original projection back (this is what OBIS and GBIF expect)
sfaa_sf_transform <- st_transform(sfaa_sf, crs = st_crs(x_geo))

##### getting OBIS data #####
# using bounding box variables defined above
library(spocc)
bounds <- c(minlon, minlat, maxlon, maxlat)
out <- occ(geometry = sfaa_sf_transform, from = 'obis', limit = 10000)
obis <- as.data.frame(out$obis$data)

```

```{r echo=F}
##### first transform back to the the original projection back (this is what OBIS and GBIF expect) #####
sfaa_sf_buf_5000_transform <- st_transform(sfaa_sf_buf_5000, crs = st_crs(x_geo))

##### getting OBIS data #####
# using bounding box variables defined above
library(spocc)
bounds <- c(minlon, minlat, maxlon, maxlat)
out <- occ(geometry = sfaa_sf_buf_5000_transform, from = 'obis', limit = 10000)
obisbuffer5000 <- as.data.frame(out$obis$data)

```

```{r eval=F, echo=F}
# export data files to local folder
##### export Excel file of data records to folder #####
setwd("C:/rworking/deepseatools/indata")
write.csv(as.data.frame(points_in_0_select),'DSCRTP_AOI_no_buffer.csv')
write.csv(as.data.frame(points_in_5000_select),'DSCRTP_AOI_5000m_buffer.csv')
write.xlsx(as.data.frame(obis),'OBIS_AOI_no_buffer.xlsx')
write.xlsx(as.data.frame(obisbuffer5000),'OBIS_AOI_5000m_buffer.xlsx')

```

```{r export_GIS_2, echo=F, message = FALSE, warning=FALSE, eval=F}
## create spatial points data frame from a subset of obis points
x_geo <- obis

## create spdf
coordinates(x_geo) <- c("longitude", "latitude")
proj4string(x_geo) <- "+proj=longlat +ellps=WGS84 +datum=WGS84"

## OPTIONAL: write out featureclass to geodatabase from sp object 
fgdb_path <- "C:/data/aprx/aoi/aoi.gdb"
arc.write(file.path(fgdb_path, 'obis'), data=x_geo, overwrite = TRUE)

```

```{r cache=T, eval=F, echo=F}
##### pinging GBIF for data #####
library(spocc)
bounds <- c(minlon, minlat, maxlon, maxlat)
out <- occ(geometry = sfaa_sf_transform, from = 'gbif')
gbif <- as.data.frame(out$gbif$data)

```

```{r export_GIS_3, echo=FALSE, echo = FALSE, message = FALSE, warning=FALSE, eval=F}
# create spatial points data frame from a subset of GBIF points
x_geo <- gbif

## create spdf
coordinates(x_geo) <- c("longitude", "latitude")
proj4string(x_geo) <- "+proj=longlat +ellps=WGS84 +datum=WGS84"

## OPTIONAL: write out featureclass to geodatabase from sp object 
fgdb_path <- "C:/data/aprx/aoi/aoi.gdb"
arc.write(file.path(fgdb_path, 'gbif'), data=x_geo, overwrite = TRUE)

```

# Purpose of Report

To create summaries of occurrence data to support fishery resource access decision making.  

# Online Resources

NOAA GeoPlatform interactive map: [LINK](https://noaa.maps.arcgis.com/home/webmap/viewer.html?webmap=4a558d07290542f68bca89ccfad38f1b&extent=-81.6837,27.7539,-77.4979,29.3078)

The main project folder for this analysis on Google Drive: [LINK](https://drive.google.com/drive/folders/1cRlhREcCdBFLlR0nzRaEB3GvrfoCZ4Pf?usp=sharing)

A Word version of this report is linked here: [LINK](https://drive.google.com/file/d/1PWa0v4rJm7yEOs1PtCHMS6lkvp7_x-WV/view?usp=sharing)

The source RMarkdown code used to generate this report is linked here on GitHub: [LINK](https://github.com/RobertMcGuinn/deepseatools/blob/master/code/2021_AOI_Analysis.Rmd)

# Methods

Coral and sponge occurrences from DSCRTP and other occurrences from OBIS and GBIF are summarized.  Buffer and intersect operations were performed to summarize the occurrence points within two zones.

* AOI 
* AOI+5km buffer

# Input datasets for this analysis

## DSCRTP National Database for Deep Sea Corals and Sponges

* National Database for Deep-sea Corals and Sponges version: `r db_version`: [LINK](https://drive.google.com/file/d/1JlHKVr2e_UX-N3XhtKnon5cRc_pnb4b_/view?usp=sharing)

* *Data dictionary*: [LINK](https://docs.google.com/spreadsheets/d/1YDskzxY8OF-34Q8aI04tZvlRbhGZqBSysuie39kYHoI/edit?usp=sharing)

## Area of Interest Polygons

The original SFAA polygons used in this analysis were acquired from Heather Coleman on 2021-08-02.  

### Original AOI

* AOI (SFAA_2014): [LINK](https://drive.google.com/file/d/1x6PJgjRfktyVB4cmCPRT0rAzQQjdTzL1/view?usp=sharing)

### Buffers of Original AOI

* AOI+5km buffer (SFAA_2014_buf_5000): [LINK](https://drive.google.com/file/d/1X5ZupVA0UFrELQhY0AHvJUZduWZ67rRX/view?usp=sharing)

## OBIS and GBIF Web Services 

Ocean Biogeographic Informaton System (OBIS) data accessed on 2021-08-05.

OBIS: [LINK](https://obis.org/)

# Tabular output for the analysis

* DSCRTP records within AOI: [LINK](https://drive.google.com/file/d/1m-SmF6V-LUCM77r-N0-ynIZb-m0sUgw2/view?usp=sharing)
* DSCRTP records within AOI + 5km buffer: [LINK](https://drive.google.com/file/d/1qX-9EwMRPOc5qzX60hqAmpbZNQZEQ0zT/view?usp=sharing)
* OBIS records within AOI: [LINK](https://drive.google.com/file/d/1PDK5SQVM5JgVBXk5fJFQt8aZuYYfG9X3/view?usp=sharing)
* OBIS records within AOI + 5km buffer: [LINK](https://drive.google.com/file/d/1ftF8D3TnSPqhP3rYk1yN37uGn_lr6bC3/view?usp=sharing)

# Preliminary results

## Images

Some, but not all, of the DSCRTP occurrences have associated images. Within the AOI+5km buffer we found `r length(z$CatalogNumber)` records with images within the database. These images are available within this Google Drive folder: [LINK](https://drive.google.com/drive/folders/1oOiCVRWJUc4fQQ1dec9q3APYfk1Ea4nj?usp=sharing)

*Note:* Useful information is embedded in the file name for these images. The naming convention used is as follows: 

"DSCRTP_CatalogNumber_Scientific Name_DepthInMeters_ImageID.jpg"

No occurrences with images were found directly intersecting the AOI. Also, no additional occurrences with images were found directly intersecting the AOI+20km buffer.  

OBIS images were not obtained for this analysis to date.

## Taxa within the AOI (DSCRTP National Database)

Number of occurrences within the AOI: `r length(points_in_0_select$CatalogNumber)`

### List of Coral Taxa Grouped by 'VernacularNameCategory' (from National Database)

``` {r CoralByVernacularNameCategoryTarget, echo=FALSE }
target <- as.data.frame(points_in_0_select)

sum_tbl <-
  target %>%
  filter(Phylum == "Cnidaria") %>% 
  group_by(VernacularNameCategory) %>%
  summarize(
    Taxa = toString(unique(ScientificName)),
    Records = n()) %>%
  arrange(desc(Records))
sum_tbl <- kable(sum_tbl, row.names = F, digits = 2)
sum_tbl

```

### List of Sponge Taxa Grouped by 'VernacularNameCategory' (from National Database)

``` {r SpongeByVernacularNameCategoryTarget, echo=FALSE }
target <- as.data.frame(points_in_0_select)

sum_tbl <-
  target %>%
  filter(Phylum == "Porifera") %>% 
  group_by(VernacularNameCategory) %>%
  summarize(
    Taxa = toString(unique(ScientificName)),
    Records = n()) %>%
  arrange(desc(Records))
sum_tbl <- kable(sum_tbl, row.names = F, digits = 2)
sum_tbl

```

### Expedition Details (DSCRTP)

``` {r Expedition_details_no_buf, echo=FALSE }

target <- as.data.frame(points_in_0_select)

sum_tbl <-
  target %>%
  group_by(DatasetID) %>%
  summarize(
    SurveyIDs = toString(unique(SurveyID)),
    PIs = toString(unique(PI)),
    Observation_Year = toString(unique(ObservationYear)),
    Records = n()) %>%
  arrange(desc(Records))
sum_tbl <- kable(sum_tbl, row.names = F, digits = 2)
sum_tbl

```

## Taxa within the AOI (OBIS)

Number of occurrences within the AOI: `r length(obis$datasetName)`

### OBIS: List of Taxa Grouped by Order

``` {r TaxaByOrderTargetOBIS, echo=FALSE }

sum_tbl <-
  obis %>%
  group_by(order) %>%
  summarize(
    Taxa = toString(unique(name)),
    Records = n()) %>%
  arrange(desc(Records))
sum_tbl <- kable(sum_tbl, row.names = F, digits = 2)
sum_tbl

```

### Expedition Details (obis)

``` {r Expedition_details_obis, echo=FALSE }
sum_tbl <-
  obis %>%
  group_by(dataset_id) %>%
  summarize(
    Records = n()) %>%
  arrange(desc(Records))
sum_tbl <- kable(sum_tbl, row.names = F, digits = 2)
sum_tbl

```

## Taxa within the AOI plus 5000m buffer zone (DSCRTP National Database)

Number of occurrences within the AOI+5000 meter buffer: `r length(points_in_5000_select$CatalogNumber)`

### List of Coral Taxa Grouped by 'VernacularNameCategory' (From National Database)

``` {r CoralByVernacularNameCategoryBuffer, echo=FALSE }
buffer <- as.data.frame(points_in_5000_select)
sum_tbl <-
  buffer %>%
  filter(Phylum == "Cnidaria") %>% 
  group_by(VernacularNameCategory) %>%
  summarize(
    Taxa = toString(unique(ScientificName)),
    Records = n()) %>%
  arrange(desc(Records))
sum_tbl <- kable(sum_tbl, row.names = F, digits = 2)
sum_tbl
```

### List of Sponge Taxa Grouped by 'VernacularNameCategory' (From National Database)

``` {r SpongeByVernacularNameCategoryBuffer, echo=FALSE }
sum_tbl <- 
  buffer %>%
  filter(Phylum == "Porifera") %>% 
  group_by(VernacularNameCategory) %>%
  summarize(
    Taxa = toString(unique(ScientificName)),
    Records = n()) %>%
  arrange(desc(Records))
sum_tbl <- kable(sum_tbl, row.names = F, digits = 2)
sum_tbl
```


### Expedition Details (DSCRTP)

``` {r Expedition_details_buf_5000, echo=FALSE }
sum_tbl <-
  buffer %>%
  group_by(DatasetID) %>%
  summarize(
    SurveyIDs = toString(unique(SurveyID)),
    PIs = toString(unique(PI)),
    Observation_Year = toString(unique(ObservationYear)),
    Records = n()) %>%
  arrange(desc(Records))
sum_tbl <- kable(sum_tbl, row.names = F, digits = 2)
sum_tbl

```

## Taxa within the AOI plus 5000m buffer zone (OBIS)

Number of occurrences within the AOI: `r length(obisbuffer5000$datasetName)`

### OBIS: List of Taxa Grouped by Order

``` {r TaxaByOrderBufferOBIS, echo=FALSE }

sum_tbl <-
  obisbuffer5000 %>%
  group_by(order) %>%
  summarize(
    Taxa = toString(unique(name)),
    Records = n()) %>%
  arrange(desc(Records))
sum_tbl <- kable(sum_tbl, row.names = F, digits = 2, align = )
sum_tbl

```


### Expedition Details (obis)

``` {r Expedition_details_obis_buffer, echo=FALSE }
sum_tbl <-
  obisbuffer5000 %>%
  group_by(dataset_id) %>%
  summarize(
    Records = n()) %>%
  arrange(desc(Records))
sum_tbl <- kable(sum_tbl, row.names = F, digits = 2)
sum_tbl

```
















