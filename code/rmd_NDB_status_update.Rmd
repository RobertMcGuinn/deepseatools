---
title: 'NOAA National Database for Deep-Sea Corals and Sponges'
output: 
    word_document:
      reference_docx: c:\rworking\deepseatools\wordtemplates\mytemplate.docx
editor_options: 
  chunk_output_type: inline
---
``` {r install_packages, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)

install.packages("ggplot2", dependencies = TRUE)
library(ggplot2)
install.packages("tidyverse", dependencies = TRUE)

#library(dplyr)
install.packages("rmarkdown", dependencies = TRUE)
library(rmarkdown)
library(knitr)
install.packages("flextable")
library(flextable)
library(googlesheets4)
library(googledrive)
install.packages("openxlsx")
library(openxlsx)
library(scales)
library(extrafont)
loadfonts()
library(RColorBrewer)
install.packages("arcgisbinding", dependencies = TRUE)
library(arcgisbinding)
install.packages("sf")
library(sf)


```

``` {r setup_opts, echo=FALSE, warning=FALSE, message=FALSE}
##### knitr options #####
knitr::opts_chunk$set(
  fig.width = 8.0, 
  fig.asp = .5, 
  warning = FALSE,
  message=FALSE,
  cache = TRUE,
  dpi = 300, 
  dev = c('png'), cache.lazy = FALSE, 
  fig.path="C:/rworking/deepseatools/reports/2019_status_update_report/figures/") 


##### setting digit options #####
options(scipen=10000)
options(digits = 0)

```

``` {r data_intake, echo=FALSE, warning=FALSE, message=FALSE, eval = T}
##### input: latest version of NDB #####
setwd("C:/rworking/deepseatools/indata")
indata<-read.csv("DSCRTP_NatDB_20191217-0.csv", header = T)
filt <- indata %>%
  filter(Flag == "0")
```

``` {r data_prep, echo=FALSE, warning=FALSE, message=FALSE}
##### data improvements (from version DSCRTP_NatDB_20191217-0)#####

filt <- filt %>% mutate(DataProvider =
                          ifelse(DataProvider == "California Academy of Sciences (CAS)",
                                              'California Academy of Sciences',
                                              as.character(DataProvider)))

filt <- filt %>% mutate(DataProvider =
                          ifelse(DataProvider == "NOAA, Office of Exploration and Research | Hawai’i Undersea Research Laboratory, University of Hawai’i", "NOAA, Office of Exploration and Research | Hawai'i Undersea Research Laboratory, University of Hawai'i", 
                                 as.character(DataProvider)))

filt <- filt %>% mutate(DataProvider =
                          ifelse(DataProvider == 'Ross (2012)', 'Harbor Branch Oceanographic Institute', as.character(DataProvider)))


##### Patch for problems with EntryDate
## bring in patch file. Raw from David Sallis on 2020-03-10 #####
## edited by RPMcGuinn on 2020-03-10 
yo <- read.xlsx('../indata/20200310-0_entrydate_fix_RPMcGuinn.xlsx')
yo <- yo %>% dplyr::select(-EntryDate)

## check
# setdiff(yo$DatasetID, filt$DatasetID)
# filt %>% filter(grepl('NOAA_CT-13', DatasetID)) %>% pull(DatasetID) %>% unique()

##### mutate step to add missing EntryDate values #####

## merge corrections using DatasetID as a key field
filt <- merge(filt, yo, all.x = TRUE)

## create new variable
filt <- filt %>% 
  mutate(EntryDate2 = if_else(is.na(as.character(EntryDate)), 
                              EntryDate_txt, 
                              as.character(EntryDate)))

## put corrected values into the correct position 
filt$EntryDate <- filt$EntryDate2

## get rid of the extra EntryDate field
filt <- filt %>% select(-EntryDate2, -EntryDate_txt)

##### checker #####

# length(filt1$CatalogNumber)

# sum_tbl <- filt %>%
#    # filter(as.numeric(IndividualCount) > 100,
#    #        grepl('Etnoyer', PI),
#    #        DatasetID == 'NOAA_SJ-10-08'
#    #       ) %>%
#   group_by(AccessionID) %>% 
#   summarize(DataProvider = toString(unique(DataProvider)),
#             Repository = toString(unique(Repository)),
#             Vessel = toString(unique(Vessel)),
#             PI = toString(unique(PI)),
#             SampleID = toString(unique(SampleID)),
#             ScientificName = toString(unique(ScientificName)),
#             ImageFilePath = toString(unique(ImageFilePath)),
#             Records = n()
#   ) %>% 
#   arrange(desc(Records))
# View(sum_tbl)
```

``` {r merge_key, eval=T}
x <- drive_find(q = "name contains '20191217-0_DatasetID_Key_DSCRTP'")

# # browse to it
# x %>% drive_browse()

# getting the id as a character string
y <- x$id

# this downloads the file to the specified path
dl <- drive_download(as_id(y), path = "C:/rworking/deepseatools/indata/20191217-0_DatasetID_Key_DSCRTP.xlsx", overwrite = TRUE)

# read the file into R as a data frame
key <- read.xlsx(dl$local_path)
merge <- merge(filt, key, all.x = TRUE)
```

``` {r schema, echo=FALSE, cache = FALSE, warning=FALSE, message=FALSE}
##### download Google Sheet version of schema for use in R  documents ##### 
# Register and download Google Sheet
options(gargle_oauth_email = "robert.mcguinn@noaa.gov")
s <- read_sheet('https://docs.google.com/spreadsheets/d/1YDskzxY8OF-34Q8aI04tZvlRbhGZqBSysuie39kYHoI/edit#gid=450007262')

```

``` {r setup_variables, echo=FALSE, warning=FALSE, message=FALSE}
# length of unflagged records 
a <- round(length(filt$CatalogNumber), -4)

# length of unflagged coral records
n_coral <- filt %>% 
  group_by(Phylum) %>% 
  summarize(n=n()) %>% 
  pull(n) %>% nth(1)

# length of unflagged sponge records
n_sponge <- filt %>% 
  group_by(Phylum) %>% 
  summarize(n=n()) %>% 
  pull(n) %>% nth(2)

# number of DatasetID
b <- length(unique(filt$DatasetID))

# number of DataProviders
c <- length(unique(filt$DataProvider))

# number of flagged records
d <- length(filter(indata, Flag == '1')$CatalogNumber)

# percent of unflagged records with images
filt2 <- filter(filt, is.na(ImageURL) == F)
e <- round((length(filt2$CatalogNumber)/(length(filt$CatalogNumber))*100),0)

# number of unflagged records with images 
f <- length(filter(filt, is.na(ImageURL) == F)$CatalogNumber)

# stats on number of records per DatasetID
y <- count(filt, DatasetID)
g <- round(median(y$n),0)
h <- min(y$n)
i <- max(y$n)

#total number of fields in the NDB
aa <- length(s$FieldName)

#total number of published fields
bb <- s %>%
  filter(InternalUseOnly == '0')
bb <- length(bb$FieldName)

#total number of unpublished fields
cc <- s %>%
  filter(InternalUseOnly == '1')
cc <- length(cc$FieldName)

#PointHist required
dd <- s %>%
  filter(PointHist == 'R')
dd <- length(dd$FieldName)

#PointNew required
ee <- s %>%
  filter(PointNew == 'R')
ee <- length(ee$FieldName)

#PointProgram required
ff <- s %>%
  filter(PointProgram == 'R')
ff <- length(ff$FieldName)

#TransHist required
gg <- s %>%
  filter(TransHist == 'R')
gg <- length(gg$FieldName)

#TransNew required
hh <- s %>%
  filter(TransNew == 'R')
hh <- length(hh$FieldName)

#TransProgram required
ii <- s %>%
  filter(TransProgram == 'R')
ii <- length(ii$FieldName)

#TrawlHist required
jj <- s %>%
  filter(TrawlHist == 'R')
jj <- length(jj$FieldName)

#TrawlNew required
kk <- s %>%
  filter(TrawlNew == 'R')
kk <- length(kk$FieldName)

#TrawlProgram required
ll <- s %>%
  filter(TrawlProgram == 'R')
ll <- length(ll$FieldName)

version <- '20191217-0'
rm(indata)

```

# Robert P. McGuinn, Thomas F. Hourigan, Scott Cross, L. Matthew Dornback, Peter J. Etnoyer, David Sallis, Heather M. Coleman

![](C:\rworking\deepseatools\indata\images\map_portal_screen_shot.png)  

####  
####  
####  

![](C:\rworking\deepseatools\indata\images\DOC_seal.jpg)

#### U.S. Department of Commerce  
#### National Oceanic and Atmospheric Administration  
#### National Marine Fisheries Service    
####   
#### NOAA Technical Memorandum NMFS-OHC-006  
#### April, 2020   

#####

#####

# 2020 Status Update: NOAA'S National Database for Deep-Sea Corals and Sponges

McGuinn, Robert P. ^4,2^; Thomas F. Hourigan ^1^; Scott Cross ^2^; L. Matthew Dornback ^5,6^; Peter J. Etnoyer ^3^; David Sallis ^4,2^; Heather M. Coleman ^1^

1. NOAA, National Marine Fisheries Service (NMFS), Office of Habitat Conservation, Deep Sea Coral Research and Technology Program (DSCRTP)  

2. NOAA, National Environmental Satellite, Data, and Information Service (NESDIS), National Centers for Environmental Information (NCEI)  

3. NOAA, National Centers for Coastal Ocean Science (NCCOS)  

4. Mississippi State University, Northern Gulf Institute (NGI), a NOAA Cooperative Institute

5. NOAA, Office of Exploration and Research  

6. Cherokee Nation Technologies  

* corresponding authors (McGuinn, Robert P. ^4,2^; Thomas F. Hourigan ^1^)  

####
####
####
####
#### NOAA Technical Memorandum NMFS-OHC-006  
#### April, 2020  


![](C:\rworking\deepseatools\indata\images\noaa_logo_bw.png)  
    
#### U.S. Department of Commerce   
#### Wilbur L. Ross, Jr., Secretary
####    
#### National Oceanic and Atmospheric Administration    
#### RDML Tim Gallaudet, Ph.D., USN Ret., Acting NOAA Administrator   
####  
#### National Marine Fisheries Service    
#### Chris Oliver, Assistant Administrator for Fisheries    

#####

**Recommended citation:**
  
McGuinn, RP, TF Hourigan, S. Cross, LM Dornback, PJ Etnoyer, D. Sallis, and HM Coleman. 2019. NOAA's National Database for Deep-Sea Corals and Sponges: Status Update. NOAA Tech. Memo. NMFS-OHC-006, XX p.
  
**Copies of this report may be obtained from:**
  
Heather Coleman  
NMFS Office of Habitat Conservation  
National Oceanic and Atmospheric Administration   
1315 East-West Highway, Room 14201  
Silver Spring, MD 20910
  
Or online at:  
http://spo.nmfs.noaa.gov/tech-memos/ or http://deepseacoraldata.noaa.gov/libary/    
  
#####    
  
# Executive Summary  

The National Oceanic and Atmospheric Administration (NOAA) Deep Sea Coral Research and Technology Program (Program) compiles, curates, and makes publically accessible a National Database of biogeographic data and information on deep-sea corals and sponges – the most important habitat-forming organisms in the deep sea. The database is accessible through the Program's Data Portal (https://deepseacoraldata.noaa.gov/) and provides geographically-specific information on the diversity, distribution and abundance of corals and sponges. The database focuses on records of occurrences within the United States Exclusive Economic Zone (US-EEZ), but the Program includes records outside of the US-EEZ where available and is actively encouraging partnerships worldwide to enhance the geographic scope of the data served. The database fulfills NOAA's requirements under the reauthorized Magnuson-Stevens Fishery Conservation and Management Act (MSA, 2007) to identify and map locations of deep-sea corals, and to submit this information for use by Regional Fishery Management Councils (FMCs). The database is the most comprehensive, quality-assured, national-scale, data portal for deep-sea corals and sponges available.
  
This report presents information on the development and enhancements to the database since it became accessible through the online portal in 2015 (Hourigan et al., 2015). Through the sustained partnership and funding activities of the Program, and the contributions of `r c` unique data providers, this database now includes over `r prettyNum(a, big.mark = ",")` records of deep-sea corals (n = `r prettyNum(n_coral, big.mark = ",")`) and sponges (n = `r prettyNum(n_sponge, big.mark = ",")`) world-wide (at database version: `r version`) along with extensive metadata. The database is now being used as an essential trusted input to open ocean planning, management, and conservation activities at the Regional Fishery Management Councils and beyond.
  
The objectives of this report are to:
  
* Provide an update on the continued development of the database through time in the form of a summary of its current contents for key variables. 

* Identify enhancements and additions to the database and related data products since 2015.  These include, *inter alia*, the following:

  + Summaries and metadata for each data set (`r b` data sets from `r c` unique data providers to date);  

  + Further standardization of coral taxonomy;    

  + Institution of additional fields that accommodate standardized habitat descriptions, comparisons of recorded depths to modeled bathymetry, additional information on areas surveyed, morphospecies descriptions, and highlight exemplary images of taxa.

* Describe quality control and assurance processes used to build and maintain the database.    

* Acknowledge the many contributors to the database (as of the date of this publication). 

* Direct data providers and data users to additional online resources.    
  
#####  

# 2020 Status Update: NOAA's National Database for Deep-Sea Corals and Sponges

####  

## Background  

Corals and sponges are the most important organisms creating deepwater biogenic habitats, and have become a centerpiece of deep-sea conservation in both the United States and internationally (Hourigan et al. 2017). The National Oceanic and Atmospheric Administration (NOAA) Deep Sea Coral Research and Technology Program (DSCRTP), herein referred to as the 'Program', compiles, curates, and makes publically accessible a National Database of biogeographic data and information on deep-sea corals and sponges. The database contains observations (records) of corals and sponges (primarily those deeper than 50 m), location and depth, and other relevant data that may be available, such as their size, abundance, and associated environmental conditions, as well as information on how the data were collected. Hourigan et al. (2015) provides an introduction and overview of the National Database.

The database is designed to be a resource for both scientists and natural resource managers, and is a primary tool linking the Deep Sea Coral Research & Technology Program’s research to management. The database fulfills NOAA's requirements under the reauthorized [*Magnuson-Stevens Fishery Conservation and Management Act*](https://www.fisheries.noaa.gov/resource/document/magnuson-stevens-fishery-conservation-and-management-act) (MSA Sec. 408, 2007) to identify and map locations of deep-sea corals, and to submit this information for use by Regional Fishery Management Councils (FMCs). See http://www.fisherycouncils.org for an overview of the structure and function of the FMCs. The database focuses on deep-sea corals and sponges within the United States Exclusive Economic Zone (U.S.-EEZ), but increasingly includes records outside of the US-EEZ where available. To enhance interoperability with other global biological occurrence databases, the field names, definitions, and valid values adhere to [Darwin Core standards](https://dwc.tdwg.org/terms/) where possible. The Program is actively encouraging partnerships worldwide to enhance the geographic reach of the data served. To encourage these partnerships and to further data awareness and usage globally, the database is also shared with the Ocean Biogeographic Information System, USA, [OBIS-USA](https://www.usgs.gov/core_science_systems/sas/obis-usa), and the Global Biodiversity Information Facility, [GBIF](https://www.gbif.org/) on a quarterly basis.

The database was originally conceived in 2009, building upon an initial United States Geological Survey Cold-Water Coral Geographic Database that was limited to Atlantic coral records (Scanlon et al., 2010). NOAA's database was developed, and continues to be refined, in collaboration with data providers, data users from the science and resource management communities, and organizations involved in similar data efforts. Since 2012, the database has undergone rapid development experiencing periods of exponential growth during major data harvests (**Figure 1**). From 2012 to 2015 was a period of internal growth and development of the database, including the establishment of new field names, definitions, and accepted (valid) values. Additionally, a web-based mapping and data download site was established and is available at DeepSeaCoralData.NOAA.gov. Online access to the database, through the DSCRTP map portal, began in the fall of 2015. Growth from 2015 and onward was marked by quarterly data updates made available to the public. The quarterly database updates include the addition of new occurrences as well as improvements to existing records.
  
The database includes `r bb` publically available fields that relate to the location, taxonomy, environmental conditions, and the provenance of the observations. Not all records include values for all available fields. The completeness of the individual records depends on completeness of the incoming data set. All of the values in this report are based on version `r version` of the National Database. Some of the reported values will change as the database is updated each quarter with new records and corrections to old records. There are currently over `r prettyNum(a, big.mark = ",")` individual records in the database composed of `r length(unique(filt$DatasetID))` individual data sets from `r length(unique(factor(filt$DataProvider)))` different data providers. There are currently `r prettyNum(n_coral, big.mark = ",")` records of corals and `r prettyNum(n_sponge, big.mark = ",")` sponge records. See **Table 1** for a full summary of all of the data providers.

``` {r GrowthThroughTime, echo=FALSE, dpi=300}
library(scales)
# ordering factors

filt2 <- filt %>%
  #filter(as.Date(EntryDate) > as.Date('2010-01-01')) %>% 
  dplyr::mutate(FishCouncilRegion = replace_na(as.character(FishCouncilRegion), "International")) %>% 
  dplyr::mutate(Phylum=recode(Phylum, 
                         'Cnidaria' = "Corals",
                         'Porifera' = "Sponges"))


## reorder levels of factors to manipulate the facet_wrap properties
filt2$FishCouncilRegion = factor(filt2$FishCouncilRegion)
filt2$FishCouncilRegion = 
  factor(filt2$FishCouncilRegion, levels = c("Caribbean", "Gulf of Mexico", "South Atlantic", "Mid-Atlantic", "New England", "North Pacific", "Pacific","Western Pacific", "International"))
    
## set options to remove scientific notation
options(scipen=10000)

x <- filt2 %>% filter (Phylum == "Corals") %>% 
  mutate(EntryDate = replace_na(as.character(EntryDate), '2019-12-18')) %>% 
  mutate(EntryDate = as.factor(EntryDate)) %>% 
  mutate(as.Date(EntryDate))

y <- filt2 %>% filter (Phylum == "Sponges") %>%
  mutate(EntryDate = replace_na(as.character(EntryDate), '2019-12-18')) %>% 
  mutate(EntryDate = as.factor(EntryDate)) %>% 
  mutate(as.Date(EntryDate))
 
ggplot(filt2,aes(x=as.Date(EntryDate), color=Phylum)) +
  stat_bin(data = x,
           aes(y=cumsum(..count..)), 
           geom="line", 
           binwidth = 2, 
           size = 1) +
  stat_bin(data = y,
           aes(y=cumsum(..count..)),
           geom="line", 
           binwidth = 2, 
           size = 1) +
  ylab("Cumulative Number of Records") + 
  xlab("Year Record Added") + 
  xlim(as.Date('2012-01-01'), as.Date('2020-01-01')) +
  scale_y_continuous(labels = comma) +
  guides(fill=FALSE) + 
  theme_bw() +
  theme(text=element_text(size=14,  family="Cambria")) +
  theme(axis.text.x = element_text(angle = 90)) +
  theme(legend.title = element_blank()) +
  theme(axis.title.x = element_text(vjust=-2.5)) +
  theme(plot.margin = unit(c(1,1,1,1), "cm"))+
  scale_color_manual(values = brewer.pal(12, "Paired")[c(10,9)])
  

## save figure if needed

# ggsave(paste("c:/rworking/deepseatools/images/", "20200404-0_","all", ".png", sep = ''), 
#        width = 6.5, 
#        height = 5.57, 
#        units = "in")
```

#### Figure 1: Growth of NOAA's National Database for Deep-Sea Corals and Sponges through time.

This document is a companion and update to an earlier NOAA Technical Memorandum entitled "An Introduction to NOAA's National Database for Deep-Sea Corals and Sponges" (Hourigan et al., 2015). The 2015 document is available online at: https://deepseacoraldata.noaa.gov/internal-documents/program-guidance/science-team-guidance-for-data-management/database_intro. Readers are referred to that document for more detail on the Program and its mandates, database structure, and on general principles that informed the design of the database and related data. The objectives of this status update are to:  

* Acknowledge contributors to the database (as of the date of this publication).

* Describe quality assurance and control (QA/QC) processes used to build and maintain the database.

* Provide an update on the continued development of the database through time in the form of a summary of its current contents for key variables.

* Direct data providers and data users to additional online resources.


**Note to readers:**    
Throughout this document, the term 'DSCRTP web portal' is used to refer to the Program's general web portal at https://deepseacoraldata.noaa.gov/. The term, 'DSCRTP map portal' refers specifically to the interactive map and data download functionality available here: https://www.ncei.noaa.gov/maps/deep-sea-corals/mapSites.htm    
  
Throughout this document, when referring to a field name from the National Database, it will be shown in single quotes exactly as it appears in the database, for example, 'DataProvider'. These field names are also used in the tables and figures for consistency and to familiarize the reader with the vocabulary used in the database. For a full list of all the field names, their definitions, and the valid values see **Appendix 1**. For convenience, the most current Excel spreadsheet version of the data dictionary is located on the DSCRTP web portal at this [URL](https://deepseacoraldata.noaa.gov/internal-documents/program-guidance/science-team-guidance-for-data-management/20170707.xlsx/view). Please note data dictionary version number. The most recent version of the data dictionary published online at that link should be used instead of the version published in this document as changes may have occurred post-publication.

## Overview of Changes to Data Working Group Workflows

Since the first online publication of NOAA's National Database for Deep-Sea Corals and Sponges in 2015, the Program has continued to develop and implement workflows to increase the number of observations, to improve the quality of existing records, and to add related data products such as Geographic Information System (GIS)-formatted data layers. The following section describes the main changes and additions to the Data Working Group workflows and processes.  

**Quarterly updates** 

The Program's Data Working Group updates the National Database with new observations and improvements to existing records on a quarterly basis, rather than handling changes in an ad hoc manner as before. The quarterly release cycle allows for better internal coordination and external communication regarding database improvements and anticipated additions and changes.    
  
New versions of the database are published on the DSCRTP map portal. The most recent database version number is indicated in the bottom left corner of the DSCRTP map portal screen as a date expressed in the format 'YYYYMMDD_X' where 'X' is an integer that captures additional iterations for a given version as needed. Older versions of the database are kept in perpetuity and are available upon request from the Program. The Program also uses these older versions to return individual records to an earlier time point if errors are recognized. The database is also archived annually at the NOAA National Centers for Environmental Information (NCEI) at the following location: https://data.nodc.noaa.gov/cgi-bin/iso?id=gov.noaa.nodc:0145037. *It is important to note* that the most current National Database is always the version available at the [DSCRTP map portal](https://www.ncei.noaa.gov/maps/deep-sea-corals/mapSites.htm) and not at the preceding NCEI archive link. 

**Automated quality control**  
  
All data submissions are subject to automated quality assurance and control routines written in both R and Python scripts that check for potential errors in depth, geographic location, and taxonomy. This is a first step in a more extensive process that includes hands-on review of each data set. More details on the quality assurance/quality control (QA, QC) processes are detailed below in the section entitled 'Overview of Data Ingestion and Quality Assurance Processes'  

**Standardized taxonomy**  

The Program has implemented a master taxonomy table to standardize all taxonomic information in the National Database. The master taxonomy table is expanded as new taxa are acquired. All taxonomic information is based on the standards maintained at the World Register of Marine Species (WoRMS) database (Horton et al., 2018). Our taxonomic list is a subset of the WoRMS database filtered to include only the organisms that are within the taxonomic scope of the National Database for Deep-Sea Corals and Sponges. For practical purposes, the database initially deviated from the WoRMS standard taxonomy by retaining the cnidarian Order Gorgonacea (generally subsumed in the Order Alcyonacea). In the 2019 update, the database has become fully compliant with WoRMS; gorgonians have been included in the Order Alcyonacea and the structural aspects of Gorgonacea have been captured in a separate field ('VernacularNameCategory' = gorgonian coral). For a discussion on the database's taxonomic scope please refer to Hourigan et al., 2015

**Standard metadata describing component data sets** 
  
For the purposes of attribution, maintaining clear provenance, and facilitating QA/QC and end-user confidence, data are identified and tracked as discrete data sets. The Program recognizes four different categories of data set: (1) data from a specific cruise, 'cruise'; (2) data from an identifiable multiyear program, such as a fisheries survey series, 'program'; (3) records from museum collections 'repository'; and (4) records from the published literature, 'literature'. As of this writing there are `r b` data sets as defined by the Program; this number grows with each quarterly update (**Table 1**). International Standards Organization (ISO) compliant metadata for each component data set are published on the DSCRTP web portal. The 'DatasetID' field is used to identify unique data sets as defined by the program. See **Appendix 3** of this document for details on accessing summary dashboards for each 'DatasetID' and downloading individual data sets and metadata. 
  
**Changes to field definitions and valid values**   

Changes are occasionally needed to make improvements to database fields, definitions, and valid values. The details of these changes are given below in the section, 'Important Changes and Additions to Database Fields'.

**Availability of related content**  
  
Some types of content are added to the DSCRTP web portal as they become available, such as Site Characterization Reports from Program-funded cruises, as well as new and improved models of habitat suitability for various taxa of coral. Both products are available for viewing and download from the DSCRTP map portal, along with details regarding model creation. The Program is committed to supporting the development and distribution of these ever-improving models for data visualization and exploration as they become available.

#####  

## Database Contributors  
  
The National Database contains contributions from museums, research programs, research cruises, and literature (**Table 1**). The database currently contains data contributions from `r c` unique data providers.

#### Table 1: Data providers to NOAA's National Database for Deep-Sea Corals and Sponges. `r c` data providers and `r b` individual data sets. The database version used for the creation of this table is `r version`
 
  
``` {r dataprovider_table, echo=FALSE}
x <-
  merge %>%
  group_by(DataProvider, class) %>% 
  dplyr::summarize(
    DatasetIDs = paste(unique(DatasetID), collapse= " | "),
    Records = n()
  )

colkeys <- col_keys <- c("Records")

ft <- flextable(x) %>% 
  theme_box() %>% #booktabs, vanilla, box, tron, tron_legacy
  colformat_num(col_keys = colkeys, digits = 0, big.mark = ",", na_str = "NA") %>% 
  font(fontname = "Cambria", part = "all") %>% #header, body, all
  fontsize(size = 10) %>% 
  flextable::width(j = ~ DatasetIDs + DataProvider, width = 2.5) %>%
  flextable::width(j = ~ class, width = .9) %>%
  merge_v(j = c('DataProvider')) %>% 
  set_header_labels(class = "Type of Data",
                    Records = "# of Records")
ft


```

#####    
    
## How to Contribute to the National Database

The Program strives to establish direct communication with data providers prior to submission to the National Database. Individuals who wish to submit coral and sponge occurrence data are encouraged to first contact the Data Working Group directly at deepseacoraldata@noaa.gov to engage prior to the data assembly and submission process. The Program is available to discuss the format and structure of the database in detail and answer any questions. The Program's Data Working Group is also available to review sample products before final submission. A set of web-based resources to assist data providers is available on the DSCRTP web portal at the following link: https://deepseacoraldata.noaa.gov/internal-documents/program-guidance/guidance-for-fieldwork-data-and-reporting  
  
Contributors to the National Database should consult **Appendix 2** of this document for a discussion of the required and desired fields for different types of data providers and research efforts. This information is also included on the database record submission template available at: https://deepseacoraldata.noaa.gov/internal-documents/program-guidance/science-team-guidance-for-data-management/dscrtp-database-records-submission-template.xlsx/view  

#####  

## Why Contribute Data to the National Database?  
  
Contributing data to NOAA's National Database for Deep-Sea Corals and Sponges can accomplish the following goals:  
  
* Provide a permanent and widely-accessible repository that serves diverse stakeholders interested in the science, management, and conservation of deep-water ecosystems, including scientists, resource managers, policy makers, economists, and educators.    
* Meet the requirements of scientific journals to provide access to underlying data sets.    
* Meet the mandate of U.S. federally-funded research to provide public access to research results, as well as the data-sharing requirements of numerous funding agencies.    
* Allow you to easily link to and share your data sets online, helping to increase visibility of your work and increase your citation rate.    
* If you have still images that are associated with your records the Program will host and link to those through the DSCRTP mapping portal, making them available for evaluation by other researchers.      
* Allow your data to easily integrate with other national and international databases. The Program provides annual uploads of the database to the Ocean Biogeographic Information System (OBIS) and the Global Biodiversity Information Facility (GBIF) through the U.S. OBIS node (OBIS-USA).    
* Help with the quality checking of existing database records.   
* Save money by using established NOAA infrastructure to host and serve your data.    
* Ensure that your data are thoroughly documented and permanently archived at NOAA's National Centers for Environmental Information.      
* Stimulate funder interest and continued research in your region.    
* Become part of the largest deep-sea coral and sponge data community in the world!   

##### 
  
## Overview of Data Ingestion and Quality Assurance Processes
  
The Program is intent on building a high-quality, value-added aggregation of deep-sea coral and sponge occurrence data that overcomes the limitations of individual incoming data sets by placing them in the context of other resources. Incoming data are checked against valid values for taxonomy, vessel names, vehicle names, and data providers. A systematic QA/QC process matches incoming values against standard value sets and thereby identifies, flags, and corrects errors where possible. All changes are documented and all previous versions of each data set, starting from the original file, are saved on NOAA computer systems for provenance tracking. The intention of the information presented below is to inform and update the reader on the Program's latest quality assurance procedures. 

### Initial Intake Inspection and Documentation Phase  

* The original data set is received from the data provider and archived as a 'Level 0' data product. The Program's data submission template facilitates data entry and streamlines incorporating data into the database.  
* Level 0 data are then saved in a uniform folder structure on NOAA systems with notes and correspondence from the data provider.   
* Data sets are assigned a standard file name and assigned an 'AccessionID', which tracks each data set received, along with any changes that occur between initial submission to publication.  
* Data sets are reviewed by Program staff familiar with deep-sea corals and NOAA data protocols. Additional notes on the submission are captured and filed. Additional communication with the data provider is initiated, if needed, to correct recognized errors or to clarify certain values.  
* All changes from the original are documented and tracked in an issue tracking system and all subsequent versions of the data are saved so that incorrect transformations or corrections can be easily reversed.  

####  

### Initial conformation
  
* Data undergo a coded QA/QC process using R and Python scripts.  

* Initial automated routines are performed to make sure data conform to the Program's data dictionary (see **Appendix 1**).   

* These automated routines accomplish the following tasks:  
  
+ Check field names and transform them where necessary.    

+ Add any missing field names to complete the schema structure.      

+ Populate null values with the appropriate entries, '-999' for integer and 'NA' for text.    

+ Rearrange the data set in a standardized column order.    

#### 

### Technical review phase
Automated quality assurance scripts are run and manual review steps are completed that include the following:

* The data are evaluated regarding how well they conform to the specific valid values listed in the data dictionary (see **Appendix 1** for the full data dictionary).  
* Taxonomic problems with any incoming 'ScientificName' are identified and corrected based on the master taxonomy table values which are synced with the World Register of Marine Species (WoRMS) database.  
* A standardized master taxonomy table is joined with the 'ScientificName' values to add accepted values for Phylum, Order, Family, etc.  
* Depths are checked against the best available regional bathymetry and mismatched values are flagged where they are out of the expected range.   
* A visual summary referred to as a 'QA dashboard' is generated using R Markdown scripting. The dashboard visualizes the values in each field in such a way as to summarize and highlight possible issues. The dashboard report also provides maps of the positions of all occurrences.  
* All issues recognized during technical review are listed and corrected if possible.  
* If an issue is not correctable, then the offending records are flagged and not released to the public.  
* A final automated QA dashboard is generated to inform the next stage of the process, expert review.  

#### 

### Expert review phase
  
* A subject matter expert conducts a final review using the information gleaned from the technical review, along with an additional inspection of the final QA dashboard, to find any outstanding issues that were not previously recognized.  
* All remaining issues are listed and corrected.  
* The expert reviewer then indicates that the data set is ready for publication in the National Database.    
    
#####
## Important Changes and Additions to Database Fields  
  
The data schema, published in 2015, has remained relatively stable, but changes have been made to enhance the utility of the database, and to improve clarity of the field definitions set forth in Hourigan et al., 2015. The full data dictionary for the National Database contains `r aa` fields, of which `r bb` are published and `r cc` are unpublished and used internally by the Program for data management purposes. For a discussion of the required and desired fields for submitting data to the Program, please see **Appendix 2**.

Logical groupings of fields, called 'Field Category' in the data dictionary, reflect the higher level organization of the database and provide a convenient way to group related fields. Two new categories, "GIS Enhancement", and "Flagging" have been added to capture a suite of new fields, bringing the number of field categories from eight to ten.    

The 'Field Category' groups are defined as follows:  

* **Survey Data:** Fields that pertain to a whole survey or cruise, such as vessel name, vehicle name, or cruise number.  
* **Event Data:** Data that pertain to an entire dive or event (e.g., trawl, transect, ROV dive).  
* **Observation Data:** Fields that that hold information about an individual record.  
* **Taxonomic ID:** Fields that capture taxonomic ranks and hierarchy, along with details regarding the identification of a taxon.  
* **Environment:** Fields that describe the habitat, substrate, and oceanographic properties in the same location as the observation.  
* **Occurrence Detail:** Fields that capture further detail about each occurrence or observation.  
* **GIS Enhancement:** Fields that originate from geospatial processing by the Program that are designed to enhance and supplement the data originally submitted. Example: 'LargeMarineEcosystem'.  
* **Metadata:** Fields that describe the origin or provenance of the records.  
* **Record Keeping:** Fields that capture unique identifiers for each data set and for each individual database record, 'DatasetID' and 'CatalogNumber', respectively.  
* **Quality Flags:** Quality and usability flags defined by the Program.  
  

All of the public and private fields for are listed in **Table 2a** and **Table 2b**. For a full list of current field names in the database, their definitions, and their valid values, see **Appendix 1** of this document. The data dictionary for the program is a version-controlled document (version `r version` at the time of this writing). **Appendix 1** is arranged alphabetically to assist the reader when searching for specific fields. The field definitions and valid values are periodically updated by the Program's Data Working Group. **Important note:** The data dictionary presented in the previous technical memo, Hourigan et al. (2015), is superseded by the current version. For the most current version of the data dictionary, users should consult the DSCRTP web portal at: https://deepseacoraldata.noaa.gov/internal-documents/program-guidance/science-team-guidance-for-data-management/20170707.xlsx/view The definitions posted at that location will always supersede all other versions (including the one published in this document).
   
#### Table 2a: Public fields of NOAA's National Database for Deep-Sea Corals and Sponges grouped by 'Field Category'. The single quoting convention used in this document is not used in the table below for enhanced visual clarity.  

``` {r public_fields_table, echo=FALSE}

s$DSCRTPCategory <- factor(s$DSCRTPCategory, levels = c("Survey Data","Event Data","Observation Data","TaxonomicID","Environment","Occurrence Detail","GIS Enhancement","Metadata","Record Keeping","Flag"))

x <-
  s %>% arrange(FieldOrder) %>% 
  filter(InternalUseOnly == '0') %>%
  group_by(DSCRTPCategory) %>% 
  summarize(FieldName = paste(unique(FieldName), collapse= " | "),
            n = n())

# colkeys <- names(x[,-1])
ft <- flextable(x) %>% 
  theme_vanilla() %>% #booktabs, vanilla, box, tron, tron_legacy
  set_header_labels(FieldName = "Field Name", n = "# of Fields", DSCRTPCategory = "DSCRTP Category") %>% 
  #colformat_num(col_keys = colkeys, digits = 0, big.mark = ",", na_str = "NA") %>% 
  font(fontname = "Cambria", part = "all")  %>% #header, body, all
  fontsize(size = 11, part = "body") %>% 
  # autofit() %>% 
  width(j = ~ n, width = 1.2) %>% 
  width(j = ~ DSCRTPCategory, width = 1.5) %>% 
  width(j = ~ FieldName, width = 4) %>% 
  align(j = 1:2, align = "left", part = "all") %>% 
  align(j = 3, align = "center", part = "all")

ft

```
  
#####
  
#### Table 2b: Private fields of NOAA's National Database for Deep-Sea Corals and Sponges grouped by 'Field Category'. The single quoting convention used in this document is not used in the table below for enhanced visual clarity.  

``` {r private_fields_table, echo=FALSE}

s$DSCRTPCategory <- factor(s$DSCRTPCategory, levels = c("Survey Data","Event Data","Observation Data","TaxonomicID","Environment","Occurrence Detail","GIS Enhancement","Metadata","Record Keeping","Flag"))

x <-
  s %>% arrange(FieldOrder) %>% 
  filter(InternalUseOnly == '1') %>%
  group_by(DSCRTPCategory) %>% 
  summarize(FieldName = paste(unique(FieldName), collapse= " | "),
            n = n())

# colkeys <- names(x[,-1])
ft <- flextable(x) %>% 
  theme_vanilla() %>% #booktabs, vanilla, box, tron, tron_legacy
  set_header_labels(FieldName = "Field Name", n = "# of Fields", DSCRTPCategory = "DSCRTP Category") %>% 
  #colformat_num(col_keys = colkeys, digits = 0, big.mark = ",", na_str = "NA") %>% 
  font(fontname = "Cambria", part = "all")  %>% #header, body, all
  fontsize(size = 11, part = "body") %>% 
  # autofit() %>% 
  width(j = ~ n, width = 1.2) %>% 
  width(j = ~ DSCRTPCategory, width = 1.5) %>% 
  width(j = ~ FieldName, width = 4) %>% 
  align(j = 1:2, align = "left", part = "all") %>% 
  align(j = 3, align = "center", part = "all")

ft

```
  
    
**New Database Fields**  
  
Several additions of internal and external fields, updates to field definitions, and updates to valid values have occurred since 2015. The Program recognizes the need for field definitions to remain as stable as possible through time, but the field definitions and valid values are revisited periodically to make additions or changes in order to capture critical new incoming information, or to clarify and refine the way a field should be implemented. Typical changes include adding a new field, improving the definition of an existing field, and changing the valid value set for an existing field. Highlighted below are the key changes to the database fields that have occurred since 2015.  

The database now has a field that indicates the current version of the database. This value is essential for reporting purposes, because it allows full reproducibility of results based on a specific version of the database. 

* **'DatabaseVersion'**: `r s$FieldDescription[s$FieldName == "DatabaseVersion"]`

Four new taxonomy-related fields were added.

* **'VerbatimScientificName'**: `r s$FieldDescription[s$FieldName == "VerbatimScientificName"]`
* **'TypeStatus'**: `r s$FieldDescription[s$FieldName == "TypeStatus"]` The database currently contains over 700 records of holotypes.
* **'IdentificationVerificationStatus'**: `r s$FieldDescription[s$FieldName == "IdentificationVerificationStatus"]`
* **'LifeScienceIdentifier'**: `r s$FieldDescription[s$FieldName == "LifeScienceIdentifier"]`

Several new fields have been added that relate to capturing and reporting morphological descriptions of taxa and highlight images.

* **OperationalTaxonomicUnit**: `r s$FieldDescription[s$FieldName == "OperationalTaxonomicUnit"]`
* **Morphospecies**: `r s$FieldDescription[s$FieldName == "Morphospecies"]`
* **CombinedNameID**: `r s$FieldDescription[s$FieldName == "CombinedNameID"]`
* **HighlightImageURL**: ``r s$FieldDescription[s$FieldName == "HighlightImageURL"]`
* **HighlightImageFilePath**: ``r s$FieldDescription[s$FieldName == "HighlightImageFilePath"]`

The previous 'Size' field was changed to 'VerbatimSize' and two new fields were added to capture the minimum and maximum size of the individual corals and sponges where these measurements are provided. 

* **VerbatimSize**: `r s$FieldDescription[s$FieldName == "VerbatimSize"]`
* **MinimumSize**: `r s$FieldDescription[s$FieldName == "MinimumSize"]` 
* **MaximumSize**: `r s$FieldDescription[s$FieldName == "MaximumSize"]`

The following fields were added to capture key habitat characteristics using standardized terminology from the Coastal and Marine Ecological Classification Standard (CMECS). https://www.cmecscatalog.org/cmecs/index.jsp .  

* **CMECSGeoForm**: `r s$FieldDescription[s$FieldName == "CMECSGeoForm"]`
* **CMECSSubstrate**: `r s$FieldDescription[s$FieldName == "CMECSSubstrate"]`
* **CMECSBiotic**:   `r s$FieldDescription[s$FieldName == "CMECSBiotic"]`

The 'EntryUpdate' field was added to capture the most current date that the Program modified the record. This is in addition to the field called 'Modified', which captures the last date that the data provider modified the record.  

* **EntryUpdate**:  `r s$FieldDescription[s$FieldName == "EntryUpdate"]` 

We added the field 'ShallowFlag' to flag occurrences that are less than 50 meters deep. These occurrences are included in our database because they represent deeper taxa that can also occur at shallower depths than our typical 50 meter cutoff. 

* **ShallowFlag**:  `r s$FieldDescription[s$FieldName == "ShallowFlag"]`

Several new spatially calculated depth fields were added to help ground truth the observed depths within the database.  

* **gisCRMDepth**: `r s$FieldDescription[s$FieldName == "gisCRMDepth"]`
* **gisGEBCODepth**: `r s$FieldDescription[s$FieldName == "gisGEBCODepth"]`
* **gisEtopoDepth**: `r s$FieldDescription[s$FieldName == "gisEtopoDepth"]`

The following location related fields were added to enhance the ability to describe and standardize place names.  
  
* **gisNGIALocality**: `r s$FieldDescription[s$FieldName == "gisNGIALocality"]`
* **gisNGIADist**: `r s$FieldDescription[s$FieldName == "gisNGIADist"]`
* **gisGEBCOLocality**: `r s$FieldDescription[s$FieldName == "gisGEBCOLocality"]`
* **gisGEBCODist**: `r s$FieldDescription[s$FieldName == "gisGEBCODist"]` 

The following fields have been added to capture the actual spatially explicit boundary of the sample area footprint for each record.  These fields are mostly unpopulated in the current version (`r version`) of the database. 

* **SampleAreaInSquareMeters**: `r s$FieldDescription[s$FieldName == "SampleAreaInSquareMeters"]` 
* **footprintWKT**: `r s$FieldDescription[s$FieldName == "footprintWKT"]`  
* **footprintSRS**: `r s$FieldDescription[s$FieldName == "footprintSRS"]`

The 'ScientificName' field definition has been updated. From this writing forward, the Program will no longer require the addition of 'sp.' to incoming 'ScientificName' entries where 'TaxonRank' is at the genus level. This change was made to make our taxonomic naming conform to community standards. 

## Key Notes on Database Fields for Data Users and Providers

This section provides additional details regarding the database structure to assist both users and data providers in the proper creation and interpretation of the occurrence records within the National Database.  

### SampleID vs. TrackingID vs. CatalogNumber

The 'SampleID' field is meant to be used by the data provider as an identifier to specify the exact observation, image, or physical specimen from which the occurrence record was created. 'SampleID' is a key value that allows Program records to be traced back to the data provider's original observation. For example, for museum specimens, the 'SampleID' would be the museum's catalog number. The purpose of the 'SampleID' field is for end-users of the database to have a link back to the original data provider's observation records. Therefore the 'SampleID' must be a persistent identifier, internal to the data provider's institution or working group, that ensures individual physical samples or imagery records will be located within their own data storage systems. If appropriate, this 'SampleID' field can be repeated within the National Database records. For example, a single still image that contains two taxa would be properly represented as two separate records in the database with the same SampleID. The 'TrackingID' can be used as a secondary identification field, e.g., for a field ID number.  The 'TrackingID' field can be used, in addition to the 'SampleID' field, when two separate IDs have been used for a single sample. If only one identifier is available, use only 'SampleID'. In contrast, the 'CatalogNumber' field is a completely unique identifier created and used by the Program to locate individual records within the National Database. The 'CatalogNumber' is assigned by the Program itself, and could number in the hundreds of thousands, or millions one day. These numbers are used to uniquely identify and reference specific occurrence records only within the context of the National Database.

### Geographic Position  

Published database records have positional information in the 'Latitude' and 'Longitude' fields. The positional information can reference a point or a line. When only a single coordinate is given to represent a linear sampling event, that point is typically the start point or midpoint, with details provided in the 'LocationComments' field. When available, some records collected along transects or during trawl surveys will also have information in the following fields: 'StartLatitude', 'StartLongitude', 'EndLatitude', and 'EndLongitude'. Other fields in the database describe details about the quality of the location data: 'NavType', 'LocationAccuracy', and 'LocationComments'. 'LocationAccuracy' specifically, is important to understand uncertainty in spatial modeling and guide users on the spatial resolution of the occurrence record. Additionally, fields about the named locality of each occurrence are included based on automated spatial analysis routines: 'Ocean', 'LargeMarineEcosystem', 'gisMEOW', 'Country', and 'FishCouncilRegion'. 'Locality' is the non-standardized named location that the data provider listed in the initial data submission. Well Known Text (WKT) representations of the geographic position of occurrences can be recorded in the pair of variables: 'footprintWKT' and 'footprintSRS' (for a deeper discussion of these variables and the quantification of the sample area, see the discussion below in the section: The Sample Area).

### The Sample Area
 
The area sampled by a survey provides important information relevant to understanding the occurrence records in the database. Each record in the database represents an observation of one or more occurrences of a single taxon within a given sample area (in square meters). The sample area will vary based on whether the observation is an actual physical sample or imagery (both still images and video), or other types of direct observation (e.g., directly observed from a submersible or by trawl sampling). Sampling area means the viewable area (or trawled area), orthogonally projected onto the sea floor, represented by a polygon. The sample area could denote the area observed in a single still image, multiple still images, a video segment, or, in the case of physical samples, sometimes a single discrete point in space. In the case of trawl records, the sample area is the area trawled, which is typically determined by trawl width and length of the trawl haul.  All details regarding the methods used for the determination of horizontal position and sample area will be reported in the variable 'LocationComments'.  

The Program has instituted several new database fields to better capture the sample area. The sample area can be reported in the field 'SampleAreaInSquareMeters'. If the exact georeferenced footprint of the polygon representing the sample area is known, it can be reported in the pair of fields called 'footprintWKT' and 'footprintSRS'.  Most major GIS software provides functionality to convert and export Well-Known Text representations of sampling polygons.  The Program would like to obtain these explicit polygonal representations of the sampling area for entire video transects or for single images where possible (example: footprints of georeferenced, downward-looking photos). This type of explicit representation of the sample area will allow a better quantification of sampling effort in a given location.  

For convenience and simplicity, each record in the database, regardless of sample area shape or size, is represented on the web mapping portal as a single geographically-located point. Geographic position of this single point is indicated in the 'Latitude' and 'Longitude' fields.  

The Program places a high value on detailed quantification of the sample area, as it helps quantify total sampling effort and is essential for analysis of the density of coral and sponge communities.  

## Connection between the Ocean Biogeographic Information System, USA (OBIS-USA), the Global Biodiversity Information Facility (GBIF), and the National Database

The Program is committed to contributing to national and international aggregations of biological occurrence data. To this end, the Program contributes updated coral and sponge data quarterly to [OBIS-USA](https://www.usgs.gov/core_science_systems/sas/obis-usa), the U.S. node of OBIS, which is part of the International Oceanographic Data and Information Exchange (IODE) of the Intergovernmental Oceanographic Commission (IOC) of the United Nations Educational, Scientific and Cultural Organization (UNESCO). OBIS-USA is coordinated by the Science Analytics and Synthesis (SAS) Program of the United States Geological Survey (USGS). The National Database contributes a particular subset of fields from its database to OBIS-USA during quarterly submissions, and these fields align to the [Darwin Core standard](https://dwc.tdwg.org/terms/), the standard used by OBIS-USA and GBIF . OBIS-USA is also a participant node of the Global Biodiversity Information Facility, [GBIF](https://www.gbif.org/). **Table 3** shows the National Database fields that are contributed to these global databases. The data that is contributed to OBIS-USA is synchronized at GBIF as well. These data sharing efforts are intended to increase the global awareness, availability, and interoperability of the National Database.

* Direct links to National Database on [OBIS](https://obis.org/dataset/f5a4799e-dc24-4807-89d9-01da47d52e3b) and [GBIF](https://www.gbif.org/dataset/df8e3fb8-3da7-4104-a866-748f6da20a3c)

#### Table 3: Fields from the National Database for Deep-Sea Corals and Sponges that are contributed to the Ocean Biogeographic Information System, USA (OBIS-USA) and the Global Biodiversity Information Facility (GBIF). The corresponding DarwinCore Term is given that matches the term used within the National Database (DSCRTP Field Name). In addition to the fields listed below, a **'references'** field is created, which is a URL that provides a direct link to the full record within the National Database.

``` {r obis_table, echo=FALSE}

DarwinCoreTerm <- c('scientificName', 'scientificNameID', 'occurrenceID', 'individualCount', 'eventDate', 'decimalLatitude', 'decimalLongitude', 'coordinateUncertaintyInMeters', 'minimumDepthInMeters', 'maximumDepthInMeters', 'basisOfRecord', 'associatedMedia')

FieldName <- c('ScientificName', 'AphiaID', 'CatalogNumber', 'IndividualCount', 'ObservationDate', 'Latitude', 'Longitude', 'LocationAccuracy', 'MinimumDepthInMeters', 'MaximumDepthInMeters', 'RecordType', 'ImageURL')

x <- data.frame(FieldName, DarwinCoreTerm)

ft <- flextable(x) %>% 
  theme_box() %>% #booktabs, vanilla, box, tron, tron_legacy
  set_header_labels(DarwinCoreTerm = "DarwinCore Term",
                    FieldName = "DSCRTP Field Name"
                    ) %>% 
  font(fontname = "Cambria", part = "all") %>% #header, body, all
  fontsize(size = 10, part = "body") %>% 
  autofit() %>% 
  width(j = ~ DarwinCoreTerm, width = 2.2) %>% 
  width(j = ~ FieldName, width = 2.2) %>% 
  align(align = "right", part = 'all') 
  
ft

```

#####

## Summary of Database Contents

The following figures and statistics provide insight into the completeness and quality status of data sets included within the National Database. These results are meant to inform and caution data users about the adequacy of using the database for various analytical objectives. **Appendix 3** includes a link to an online resource (data set dashboards) that describes each unique data set in detail as well as providing an interactive map to explore those data in isolation. The data set dashboards also have a link to download the data set as well as International Standards Organization compliant metadata (ISO 19115-Parts 1 and 2).

The figures and tables that are presented in the following section are designed to give readers greater insight into the current contents and status of a few key fields in the database. For clarity and brevity, not all variables within the database are covered here. We elected to summarize fields and highlight relationships that clarify the distribution of data holdings and well as identify issues that data users may find useful for their analytical goals.

### Analysis Details  
  
The analyses presented here use version `r version` of the database. In some cases, the data have been filtered to enhance readability or interpretation of a figure or table. These filters will be called out explicitly in the text or captions when used. The purpose of this analysis is to succinctly summarize the current contents and quality of the database, so that any major limitations are apparent to users. The numerical and graphical summaries provided in this document were created using R and R-Markdown used within the RStudio development environment (R Core Team, 2017; RStudio Team, 2016). All R code is shared on GitHub at https://github.com/RobertMcGuinn/deepseatools.   

### Distribution of Records by U.S. Fishery Management Council Region  

```{r region-counts, echo=FALSE}
a <- filt %>% 
  filter(
    is.na(FishCouncilRegion) == T
    ) 
a <- length(a$CatalogNumber)/length(filt$CatalogNumber)
a <- round(a*100, 0)

```
  
The primary geographic focus of NOAA's Deep Sea Coral Research & Technology Program is in the U.S. EEZ. Primary clients for the National Database are the United States Regional Fishery Management Councils. The 'FishCouncilRegion' field identifies the council region (and adjacent state waters) within which a record occurs. **Figure 2** shows the distribution of records by region. The large number of records from the Pacific Council Region (i.e., U.S. West Coast) reflects our partnership with the Monterey Bay Aquarium Research Institute (MBARI). We also recognize the scientific value of data from around the world and hope that our database can serve as an international resource. Currently, around `r prettyNum(a, big.mark = ",")`% of records occur outside U.S. Fishery Management Council regions, either in international waters (areas beyond national jurisdiction) or within the boundaries of other countries' EEZs (**Figure 2**).

``` {r FishCouncilRegion, echo=FALSE}
# ordering factors

filt2 <- filt %>% 
  filter(is.na(Phylum) == F # , 
         #is.na(FishCouncilRegion) == F# , 
         #FishCouncilRegion != 'NA'
         )
options(scipen=10000)

#table(filt2$FishCouncilRegion, useNA = 'always')

filt2$FishCouncilRegion <- factor(filt2$FishCouncilRegion, levels = c('Pacific', 'North Pacific','Western Pacific','New England','Mid-Atlantic','South Atlantic', 'Gulf of Mexico','Caribbean'))

g <- ggplot(filt2, aes(FishCouncilRegion, fill = Phylum)) +
  geom_bar() + 
  coord_flip() + 
  theme(text = element_text(size=20)) + 
  ylab("Number of Records") + 
  theme_bw(base_size = 15, base_family = "Cambria")

#display.brewer.all(colorblindFriendly=TRUE)
g + scale_fill_manual(values = brewer.pal(12, "Paired")[c(10,9)])

# write to file
# x <- g + scale_fill_manual(values = brewer.pal(12, "Paired")[c(10,9)])
# 
# png(file = "C:/rworking/deepseatools/reports/2019_status_update_report/fishcouncilregion.png",
#     width = 1200, height = 800, 
#     units = "px", pointsize = 12, res = 150)
# 
# x
# 
# dev.off()

```

#### Figure 2: Distribution of known coral and sponge occurrences grouped by Fishery Management Council region ('FishCouncilRegion'). The number of records in a given region is primarily a measure of sampling effort and data reporting to the National Database. The counts are not meant as a reflection of the abundance of deep-sea corals or sponges within a region. 'NA' = records outside the U.S. EEZ.
####       
### Images Associated with Records   

As of the current version, approximately `r e`% or `r prettyNum(f, big.mark = ",")` records have full resolution still images available. There are `r prettyNum(length(unique(filt$ImageURL)), big.mark = ",")` unique images (i.e. some images contain multiple taxa). These images are viewable directly in the DSCRTP map portal by clicking on coral or sponge points. The 'ImageURL' field also contains the link to the image if available, and this field is available in data downloaded from the DSCRTP map portal.

### Depth Distribution  
  
'DepthInMeters' is the primary depth field in the database with all of the published records including depth information. This field records either the actual reported depth, the average depth (when separate 'MinimumDepthInMeters' and 'MaximumDepthInMeters' are reported), or, in cases where no depth is reported, a GIS-derived depth for their position is assigned, relative to the best available bathymetry. These spatially derived depths are available in the following fields: 'gisCRMDepth', 'gisGEBCODepth', and 'gisEtopoDepth' and provide an important means of checking depth. Averaged and assigned depths should be treated as approximations.

The 'DepthMethod' and 'LocationComments' fields contain additional details on the source and methodology for depth values present in the 'DepthInMeters' field. For full definitions and valid values of these fields, see **Appendix 1**. See **Figure 3** below for a distribution of depth values in the National Database by Fishery Management Council region ('FishCouncilRegion'). It is important to note here again: Distribution of depth values represented in **Figure 3** are a result of sampling effort and subsequent reporting and not necessarily a reflection of regional depth distributions of corals and sponges.

``` {r depth_region, echo=FALSE}

##### Boxplot of depth by FishCouncilRegion #####

filt2 <- filt %>% 
  filter(DepthInMeters != "-999", is.na(Phylum) == F)

# reorder levels

filt2$FishCouncilRegion <- factor(filt2$FishCouncilRegion, levels = c("Caribbean", "Gulf of Mexico", "South Atlantic", "Mid-Atlantic", "New England", "Pacific", "Western Pacific", "North Pacific"))       


g <- ggplot(filt2, aes(factor(FishCouncilRegion), as.numeric(DepthInMeters), color = Phylum)) +   geom_boxplot() +
  scale_y_reverse() +
  ylab("Depth (meters)") + 
  xlab("FishCouncilRegion") + 
  theme_bw(base_size = 15, base_family = "Cambria") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = -.01))

#display.brewer.all(colorblindFriendly=TRUE)
g + scale_color_manual(values = brewer.pal(12, "Paired")[c(10,9)])

```

#### Figure 3: Distribution of depth values by Fishery Management Council region ('FishCouncilRegion'). The 'NA' values are records that are outside of the U.S EEZ.
####    
### Observations through Time

The year in which an observation was made, 'ObservationYear', provides some insights for understanding deep sea exploration activities through time. **Figure 4** provides a visualization of the distribution of the 'ObservationYear' field. The database includes records from `r min(filt$ObservationYear)` to `r max(filt$ObservationYear)`, with the median year of observation being `r median(filt$ObservationYear)`.  Most records prior to 1980 represent museum specimens. The large increase in 21st century records reflects the increasing importance of records annotated from still images and video.

``` {r ObservationYear, echo=FALSE,  dpi = 300}
# ordering factors
filt2 <- filt %>% 
  filter(is.na(ObservationYear) == F, ObservationYear != "-999", is.na(Phylum) == F)
filt2$TaxonRank <- factor(filt2$TaxonRank, levels = c("phylum", "class", "subclass", "order", "suborder","family","subfamily", "genus","subgenus","species","subspecies","forma","variety"))
options(scipen=10000)

g <- ggplot(filt2, aes(ObservationYear, fill = Phylum)) +
  geom_bar() + 
  ylab("Number of Records")  +
  theme_bw(base_size = 15, base_family = "Cambria")

#display.brewer.all(colorblindFriendly=TRUE)
g + scale_fill_manual(values = brewer.pal(12, "Paired")[c(10,9)])

```

#### Figure 4: The temporal distribution of coral and sponge observations by 'ObservationYear.'
####    
### Individual Count

```{r individualcount_calc, echo=FALSE}

#total number of published fields
max <- filt %>%
  filter(IndividualCount != '-999')
max <- max(filt$IndividualCount)

```


The 'IndividualCount' field indicates the number individual corals or sponges per record. `r round((length(filter(filt, IndividualCount != '-999')$CatalogNumber)/length(filt$CatalogNumber))*100,2)` % or `r prettyNum(length(filter(filt, IndividualCount != '-999')$CatalogNumber), big.mark = ",")` records have 'IndividualCount' values. Most of the records fall within the range of 1-50 individuals per observation frame, but the maximum is over 16,000 individuals. The median value of 'IndividualCount' is `r median(filt$IndividualCount)`. See **Figure 5** for the distribution of individual count between 0 and 50.  

When interpreting individual count values ('IndividualCount'), it is important to consider the geographic extent of the sampling events. The database includes sampling frames that are as small as a single specimen or photograph and as large as a video transect or the area sampled during a trawl haul. For many records, the size of the sampling frame is unknown. The 'SampleAreaInSquareMeters' is a new field designed to capture the area of the sampling frame, but at the current database version (`r version`) only `r round((length(filter(filt, SampleAreaInSquareMeters != '-999')$CatalogNumber)/length(filt$CatalogNumber))*100,.1)`% of the records are populated for this variable. Where populated, this value can be used to spatially normalize the count values by area for the purpose of making calculations of density. Also, when 'IndividualCount' is not populated, species abundance values may be reported in 'CategoricalAbundance' instead, which is simply a categorical version of the count of the number of individuals counted within the sampling frame. The valid values for 'CategoricalAbundance' are: `r s %>% filter(FieldName == 'CategoricalAbundance') %>% dplyr::select(ValidValues)`. Additional fields accommodate other measures of taxon abundance can be expressed as 'WeightInKg' (used most often for reporting trawl survey results) and 'Cover' (often used for framework-forming corals such as *Lophelia pertusa* where individual colonies cannot be distinguished and counted), and 'Density'.

``` {r IndividualCount, echo=FALSE,  dpi = 300}
# ordering factors
filt2 <- filt %>% 
  filter(is.na(IndividualCount) == F, 
         IndividualCount != '0',
         IndividualCount != '-999', 
         is.na(Phylum) == F, 
         IndividualCount < 50
         )

options(scipen=10000)

g <- ggplot(filt2, aes(IndividualCount, fill = Phylum)) +
  geom_histogram(binwidth = 1) + 
  ylab("Number of Records")  +
  theme_bw(base_size = 15, base_family = "Cambria")

#display.brewer.all(colorblindFriendly=TRUE)
g + scale_fill_manual(values = brewer.pal(12, "Paired")[c(10,9)])

```

#### Figure 5: Distribution of 'IndividualCount' values between 0 and 50 corals or sponges. The figure's x-axis is truncated at 50 individuals for visual clarity.
####    
### Size

``` {r SizeCalc, echo=FALSE,  warning = FALSE}
yo <- filt %>% 
  filter(
    MinimumSize != "-999" | 
    MaximumSize != "-999"
    #MinimumSize != '0',
    #is.na(Phylum) == F
    )

#checking
# length(yo$CatalogNumber)
# yo %>% filter(as.numeric(MaximumSize) > 200) %>% dplyr::select(CatalogNumber, MinimumSize, RecordType, ScientificName)

yo$AverageSize <- (yo$MinimumSize + yo$MaximumSize)/2
yo$SizeDiff <- (yo$MaximumSize - yo$MinimumSize)

x <- length(filt$CatalogNumber)
y <- length(yo$CatalogNumber)

percent <- (y/x) * 100 

maxcoralsize <- filt %>% filter(Phylum == 'Cnidaria') %>% pull(MaximumSize) %>% max()

maxspongesize <- filt %>% filter(Phylum == 'Porifera') %>% pull(MaximumSize) %>% max()

```

Size of coral colonies and sponges is an important measure that may be associated with a structure-forming organism's value as habitat for other species. About `r round( percent,.1)`% of all records have reported size values (n = `r prettyNum(y, big.mark = ',')`) In the database, size values are reported in fields called 'MinimumSize' and 'MaximumSize' fields in order to capture a range of sizes that might be observed within given sampling area. In the case of a video transect, for example, multiple sizes may be observed for a single taxon, so reporting a range is appropriate. Even in a single still image, you may observe a range of sizes for a single taxon. See **Figure 6** below for distribution of maximum size values. The original size values, exactly as reported by the data provider, are captured in a field called 'VerbatimSize'. The maximum size reported in the database for corals is `r maxcoralsize` cm and for sponges is `r maxspongesize` cm.

``` {r MaxSize, echo=FALSE,  warning = FALSE, dpi = 300}
yo <- filt %>%
  filter(
    MaximumSize != "-999", 
    MaximumSize < 100
    )

# ordering factors
options(scipen=10000)
g <- ggplot(yo, aes(as.numeric(MaximumSize), fill = Phylum)) +
  geom_histogram(bins = 20
                 ) + 
  #coord_cartesian(ylim = c(1, 20000)) +
  ylab("Number of Records") + 
  xlab("Maximum Size (cm)")  +
  theme_bw(base_size = 15, base_family = "Cambria")

#display.brewer.all(colorblindFriendly=TRUE)
g + scale_fill_manual(values = brewer.pal(12, "Paired")[c(10,9)])

```

#### Figure 6: Histogram of 'MaximumSize' values. For clarity the x-axis limit has been set to 100 cm.

### Condition

```{r ConditionCalc, echo=FALSE}

condition <- filt$Condition %in% c('Live','Damaged','Dead')
x <- length(condition[condition == T])
y <- filter(filt, is.na(filt$Condition) == FALSE)
y <- length(y$Condition)
z <- filter(filt, is.na(filt$Condition) == TRUE)
z <- length(z$Condition)
t <- length(filt$Condition)

```

Approximately `r round((y/t) *100,.1)`% of all records have 'Condition' values reported. The currently valid values for Condition are: `r s %>% filter(FieldName == 'Condition') %>% dplyr::select(ValidValues)`. Most of the records that have 'Condition' reported have been reported as 'Live'. It is likely that this is influenced by a strong sampling and observation bias for living coral and sponges, along with a difficulty in discerning, or inconsistency in reporting, damage (**Figure 7**).  

``` {r Condition, echo=FALSE,  dpi = 300}
yo <- filt %>% 
  filter(
    Condition %in% c('Dead','Damaged','Live', NA),
    is.na(Phylum) == F
    #Condition != 'Live'
         )

# ordering factors
yo$Condition <- factor(yo$Condition, 
                       levels = c('Dead','Damaged', 'Live', NA))


options(scipen=10000)
g <- ggplot(yo, aes(Condition, fill = Phylum)) +
  geom_bar() + 
  coord_flip() + 
  ylab("Number of Records") + 
  theme_bw(base_size = 15, base_family = "Cambria")

#display.brewer.all(colorblindFriendly=TRUE)
g + scale_fill_manual(values = brewer.pal(12, "Paired")[c(10,9)])

```

#### Figure 7: Bar plot of 'Condition' values, showing most records in the database are of live coral and sponges.
####       
### Habitat and Substrate

Habitat and substrate values are in the database, verbatim, as reported by the data provider. No standard valid values have been adopted or implemented for these fields. Three newly-established fields using the Coastal and Marine Ecological Classification Standard (CMECS)  classifications (CMECSBiotic, CMECSGeoForm, CMECSSubstrate) are intended to help standardize reporting of habitat measures in the future. See the following URL for more information about CMECS. https://www.natureserve.org/conservation-tools/coastal-and-marine-ecological-classification-standard-cmecs

Records with 'Substrate' values: `r round((length(filter(filt, is.na(Substrate) == F)$CatalogNumber)/length(filt$CatalogNumber))*100, .1)`% or `r prettyNum(length(filter(filt, is.na(Substrate) == F)$CatalogNumber), big.mark = ',')` 

Records with 'Habitat' values: `r round(length(filter(filt, is.na(Habitat) == F)$CatalogNumber)/length(filt$CatalogNumber)*100, .1)`% or `r prettyNum(length(filter(filt, is.na(Habitat) == F)$CatalogNumber), big.mark = ',')` 

### Taxonomy

```{r taxonomy_fields, echo=FALSE}
z <- s %>%
  filter(DSCRTPGroup == 'Taxonomy Metadata', InternalUseOnly == "0", FieldName != 'AssociatedTaxa', FieldName != 'Synonyms')
z <-toString(z$FieldName)

```

All records in the database are populated with a complete taxonomic string based on the [World Register of Marine Species (WoRMS)](http://www.marinespecies.org/). In addition, greater information about taxonomic identifications are available in the following descriptive fields: `r z`. See **Appendix 1** for the full definitions of these fields.      
Taxonomic identifications can change through time, either through refinements in identification by taxonomists or through changes to underlying group taxonomy. The Program attempts to keep up with these changes, but these fields should be checked carefully before using the data for analysis.
  
'TaxonRank' provides taxonomic resolution of the identifications in the database (**Figure 8**) Most corals and sponges are identified to the taxon rank ('TaxonRank') of genus or species.  

``` {r taxonranksum, echo=FALSE,  dpi = 300}
# ordering factors
filt2 <- filt %>% 
  filter(is.na(TaxonRank) == F, is.na(Phylum) == F)

filt2$TaxonRank <- factor(filt2$TaxonRank, levels = c("phylum", "class", "subclass", "order", "suborder","family","subfamily", "genus","subgenus","species","subspecies","forma","variety"))
options(scipen=10000)

g <- ggplot(filt2, aes(TaxonRank, fill = Phylum)) +
  geom_bar() + 
  coord_flip() + 
  ylab("Number of Records") + 
  theme_bw(base_size = 15, base_family = "Cambria")

#display.brewer.all(colorblindFriendly=TRUE)
g + scale_fill_manual(values = brewer.pal(12, "Paired")[c(10,9)])
```

#### Figure 8: Distribution of 'TaxonRank' across all records. Corals are only included in the database if they are identified at least to Order, as higher taxonomic levels contain cnidarians that are not considered to be corals. Sponge records are accepted at the phylum level and below.
####
### Record Types

``` {r recordtype_calc, echo = FALSE}
valid <- filt$RecordType %in% c('literature',  'specimen',  'still image', 'video observation', 'video transect','notation', 'catch record')

# number of records with valid values
x <- length(valid[valid == T])

# number of records where the specified value is non-null 
y <- filter(filt, is.na(filt$RecordType) == FALSE)
y <- length(y$RecordType)

# number of records where the specified value is null
z <- filter(filt, is.na(filt$RecordType) == TRUE)
z <- length(z$RecordType)

# total number of unflagged records
t <- length(filt$RecordType)

```

`r round((y/t) *100,1)`% of all records (n = `r prettyNum(y, big.mark = ',')`) have 'RecordType' values reported. **Figure 9** shows the distribution of these values across the entire database.

``` {r RecordType_global, echo=FALSE,  dpi = 300}
yo <- filt %>% 
  filter(RecordType %in% c('literature',  'specimen',  'still image', 'video observation', 'notation', 'catch record'), is.na(Phylum) == F)

yo <- within(yo, 
             RecordType <- factor(RecordType, 
                                  levels=names(sort(table(RecordType), decreasing=TRUE))))

options(scipen=10000)
#yo <- yo %>% filter(FishCouncilRegion == 'South Atlantic'| FishCouncilRegion == 'Caribbean')
#table(yo$FishCouncilRegion)

# ordering factors
options(scipen=10000)
g <- ggplot(yo, aes(RecordType, fill = Phylum)) +
  geom_bar() + 
  coord_flip() + 
  #facet_wrap(~FishCouncilRegion) + 
  ylab("Number of Records") + 
  theme_bw(base_size = 15, base_family = "Cambria") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

#display.brewer.all(colorblindFriendly=TRUE)
g + scale_fill_manual(values = brewer.pal(12, "Paired")[c(10,9)])

```

##### Figure 9: The distribution of valid 'RecordType' values across the entire database.

### Sampling Equipment

``` {r SamplingEquipmentCalcs, echo = FALSE}
valid <- filt$SamplingEquipment %in% c('ROV', 'AUV', 'submersible', 'drop camera', 'towed camera', 'trawl', 'net', 'dredge', 'longline', 'pot', 'hook and line', 'grab', 'corer', 'SCUBA', 'epibenthic sled', 'other', 'NA')

#s %>% filter(FieldName == "SamplingEquipment") %>% dplyr::select(ValidValues)
#View(table(filt$SamplingEquipment))

# number of records with valid values
x <- length(valid[valid == T])

# number of records where the specified value is non-null 
y <- filter(filt, is.na(filt$SamplingEquipment) == FALSE)
y <- length(y$SamplingEquipment)

# number of records where the specified value is null
z <- filter(filt, is.na(filt$SamplingEquipment) == TRUE)
z <- length(z$SamplingEquipment)

# total number of unflagged records
t <- length(filt$SamplingEquipment)

```

Approximately `r round((y/t) *100,.1)`% of all records (n = `r prettyNum(y, big.mark= ',')`) have 'SamplingEquipment' values reported. Additional information on the sampling equipment (e.g., specific type of trawl) and other methods are often included in the 'SurveyComments' or in the data set metadata. See **Figure 10** below for the makeup of the entire database by 'SamplingEquipment'.

``` {r SamplingEquipmentPlot, echo=FALSE,  dpi = 300}
yo <- filt %>% 
  filter(SamplingEquipment %in% c('ROV', 'AUV', 'submersible', 'drop camera', 'towed camera', 'trawl', 'net', 'dredge', 'longline', 'pot', 'hook and line', 'grab', 'corer', 'SCUBA', 'epibenthic sled', 'other', 'NA'), is.na(Phylum) == F)

yo <- within(yo, 
             SamplingEquipment <- factor(SamplingEquipment, 
                                  levels=names(sort(table(SamplingEquipment), decreasing=TRUE))))

# ordering factors
options(scipen=10000)
#yo <- yo %>% filter(FishCouncilRegion == 'South Atlantic'| FishCouncilRegion == 'Gulf of Mexico' | FishCouncilRegion == 'Caribbean')
#table(yo$FishCouncilRegion)

g <- ggplot(yo, aes(SamplingEquipment, fill = Phylum)) +
  geom_bar() + 
  coord_flip() +
  #facet_wrap(~ FishCouncilRegion) +
  ylab("Number of Records") + 
  theme_bw(base_size = 15, base_family = "Cambria") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

#display.brewer.all(colorblindFriendly=TRUE)
g + scale_fill_manual(values = brewer.pal(12, "Paired")[c(10,9)])

```

#### Figure 10: The distribution of 'SamplingEquipment' values across the entire database.

## Caveats Regarding Use of the Data

Data users should be cautious when using these data for specific analytical purposes, especially when using data from more than one sampling effort at a time. As with all data aggregations, meta-analyses of disparate data are complex and many pitfalls can arise when combining data from sources that use different sampling methods. Users of the data are encouraged to carefully consider which data sets (and even which records within a data set) are appropriate to use for the analytical purpose at hand, especially in regard to evaluating identification and positional uncertainty. The Program has made considerable effort to assess the quality of data included in the National Database and to exclude data that are not up to standard. Additionally, numerous metadata fields are included to provide users with additional information that can be used to evaluate the data.  Despite those efforts, data quality issues do still exist in the database and users should conduct their own quality checks prior to relying on the database for particular analyses and management applications.

## Conclusion

The Program's goal is to provide broad public access to a comprehensive, consolidated, quality assured data resource for deep-sea corals and sponges in the U.S. and around the world. It is our intention that all data providers receive the credit they deserve for helping build this resource, which is designed to enhance deep-sea conservation and management decision making. By creating the National Database for Deep-Sea Corals and Sponges and the data portal at DeepSeaCoralData.NOAA.gov, the Program has provided a comprehensive and convenient one-stop source to view and access these data.

This report has summarized the current holdings and quality level of NOAA's National Database of Deep-Sea Corals and Sponges in order to enhance understanding, appropriate use, and to indicate areas for further improvement. The Program welcomes feedback from portal and data users. Please send us your comments and questions to deepseacoraldata@noaa.gov. This report, and any follow-on updates, will continue to track the ongoing evolution of this key resource for deep sea ecologists, biologists, managers, policy makers, and explorers.

## Acknowledgements

The Program would like to thank all the individuals and institutions who have contributed their data to be published in the common framework of the National Database. We recognize the considerable time and effort that it takes to collaborate with us, and for that, we are grateful. Also, thanks to all the contributors to the conceptualization and design of the database and the deep sea coral data portal. This resource is only possible with all of the individual and institutional contributors from the deep-sea research community. We would also like to thank all of the users of the data and portal for providing the real-world application of these resources. A huge thanks to all of the reviewers for their insights and improvements to this document (in alphabetical order by last name): Rachel Bassett, Abigail Benson, Just Cebrian, Elizabeth Duncan, Meredith Everett, Enrique Salgado, and Curt Whitmire. Thanks to all who continue to help us evolve these information resources to enhance deep-sea conservation and management. 

## References

Amante, C. and B.W. Eakins (2009) ETOPO1 1 Arc-Minute Global Relief Model: Procedures, Data Sources and Analysis. NOAA Technical Memorandum NESDIS NGDC-24. National Geophysical Data Center, NOAA. doi:10.7289/V5C8276M [access date].] Available from https://www.ngdc.noaa.gov/mgg/global/.

Horton, T. et al. (2018) World Register of Marine Species. Available from http://www.marinespecies.org at VLIZ. Accessed 2019-12-10. doi:10.14284/170.

Hourigan, T.F., P.J. Etnoyer, R.P. McGuinn, C. Whitmire, D.S. Dorfman, L.M. Dornback, S. Cross, and D. Sallis (2015) An Introduction to NOAA's National Database for Deep-Sea Corals and Sponges. NOAA Technical Memorandum NOS NCCOS 191. 27 pp. Silver Spring, MD. Available from https://deepseacoraldata.noaa.gov/internal-documents/program-guidance/science-team-guidance-for-data-management/database_intro.

Hourigan TF, Etnoyer PJ, Cairns SD (2017) The State of Deep-Sea Coral and Sponge Ecosystems of the United States.  NOAA Technical Memorandum NMFS-OHC-004 Silver Spring, MD 467 pp. https://deepseacoraldata.noaa.gov/library/2017-state-of-deep-sea-corals-report    

R Core Team (2017) R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. Available from  https://www.R-project.org/.
RStudio Team (2016) RStudio: Integrated Development for R. RStudio, Inc., Boston, MA. Available from http://www.rstudio.com/.

Scanlon, K.M., R.G. Waller, A.R. Sirotek, J.M. Knisel, J.J. O'Malley, and S. Alesandrini (2010) USGS cold-water coral geographic database-Gulf of Mexico and western North Atlantic Ocean, version 1.0: U.S. Geological Survey Open-File Report 2008-1351. Available from https://pubs.usgs.gov/of/2008/1351/.

 
#####
  
## Appendix 1: Data Dictionary for the National Database

National Database field names, definitions, and their valid values, organized by field name. This table includes only the public fields in the National Database. The fields are arranged alphabetically for ease of use. The current version of this table is also available as an Excel file on the DSCRTP web portal: 
https://deepseacoraldata.noaa.gov/internal-documents/program-guidance/science-team-guidance-for-data-management/20170707.xlsx/view   

**Important Note on Data Dictionary Version:** For the most current version available, access the link above. The version printed here may be out of date due to post-publication updates of the schema. Users are advised to use the latest Excel version of the table for preparing a new data for submission or for analytical guidance. 


``` {r data_dictionary_table, echo=FALSE}
x <-
  s %>%  filter(InternalUseOnly == '0') %>% 
  group_by(FieldName) %>% 
  summarize(Field_Definition = toString(unique(FieldDescription)),
            Valid_Values = toString(unique(ValidValues)))

colkeys <- names(x[,-1])
ft <- flextable(x) %>% 
  theme_box() %>% #booktabs, vanilla, box, tron, tron_legacy, zebra
  set_header_labels(Field_Definition = "Field Definition", 
                  Valid_Values = "Valid Values",
                    FieldName = "Field Name") %>% 
  colformat_num(col_keys = colkeys, digits = 0, big.mark = ",", na_str = "NA") %>% 
  font(fontname = "Cambria", part = "all")  %>% #header, body, all
  fontsize(size = 10, part = "body") %>% 
  autofit() %>% 
  width(j = ~ FieldName , width = 1.7) %>%
  width(j = ~ Field_Definition, width = 3.5) %>% 
  width(j = ~ Valid_Values, width = 2) %>%
  align(align = "left", part = 'all')
  


  #width(j = ~ DSCRTPCategory, width = 2) %>% 
  

ft

```
  
##### 
  
## Appendix 2: Required and Desired Fields for Submission of Data to the National Database

Each of the data sets in the database has unique 'DatasetID'. There are currently `r length(filt$CatalogNumber)` records in the database composed of `r length(unique(filt$DatasetID))` individual data sets from `r length(unique(filt$DataProvider))` different data providers. You will find an an interactive dashboard for each data set at the following location: LINK (not live yet, see reviewer note below) 

The following section is revised from Hourigan et al. 2015. Even though the database has a diverse array of fields that can accommodate many types of information, not all are required when submitting data to the Program. For example, the minimum number of required fields for submission to the database is eight. This number is for historical point-based records. All incoming data are evaluated according to the data type. A two-factor classification is used to characterize incoming data according to type. Three categories, 'Point', 'Transect', and 'Trawl', are used to differentiate the first factor and are defined below.

* **Point:** Discrete observations from a single geographic point. An example is an observation of a single taxon from an individual still image or alternatively an actual single point collection of a physical sample.

* **Transect**: Cumulative observations from a pre-determined linear transect. An example would be observations from multiple still images (within a transect) that are grouped together to form a single record in the National Database. Even though these records are currently represented as a single point on the DSCRTP map portal interface, they are an inherently linear sampling frame, and additional required fields help capture the start and end points of the transect line.

* **Trawl:** Specimens from trawl hauls. These data represent observations of multiple collected individuals over a single trawl-haul describing a single taxa, and represented by a single record in the National Database. These records also have an inherently linear sampling frame, and additional required fields help to capture the start and end points of the trawl line if available.
Three categories, 'Historical', 'New', and 'Program' are used to differentiate the second factor and are defined below.

* **Historical:** 'Historical' data are already collected and the Program has no ability to influence the format or observational methods. An example is a data set already collected under the funding of another program and that might already be in the literature or another database format, such as records from Monterey Bay Aquarium Research Institute and the Smithsonian Institution at the National Museum of Natural History.

* **New:** 'New' data have not yet been collected, but are intended to be submitted to the Program. An example is data from an upcoming expedition that is not funded by the Program and therefore has fewer submission requirements. The Program would like to encourage these efforts to submit their observations to the National Database, and can provide direct assistance when researchers contact the Program prior to data collection.

* **Program:** Data collected under exploration funded by the Program have the most required fields for submission. In the case of Program funded data collection, it is expected that the principal investigators coordinate directly with the Program to process their data according to Program standards outlined here. Examples include data from upcoming planned cruises with Program funding and 'data rescue' projects designed to create new annotations from existing unanalyzed video or still photos.

Used in combination, these factors (each with three categories), lead to nine different types of incoming data with different numbers of required fields. The Data Working Group uses notation to refer to these combinations: Point Historical, Point New, Point Program, Transect Historical, Transect New, Transect Program, Trawl Historical, Trawl New, Trawl Program. *Table A-C* below summarize the required fields for the nine possible incoming data types.

#####

#### Table A: Required and desired fields for different types of incoming point-based data. 'R' means the field is required for data submission and 'D' means that it is desired by the Program. 'P' means that the field is supplied by the Program.

``` {r required_desired_points_table, echo=FALSE,  warning=FALSE}
x <-
  s %>%
  filter(InternalUseOnly == '0',
         PointHist == 'R' | PointNew == 'R' | PointProgram == 'R'
         ) %>%
  group_by(DSCRTPCategory, FieldName) %>% 
  summarize(PointHist = unique(PointHist),
            PointNew = unique(PointNew),
            PointProgram = unique(PointProgram)
                        )
colkeys <- names(x[,-1])

ft <- flextable(x) %>% 
  theme_vanilla() %>% #booktabs, vanilla, box, tron, tron_legacy
 # set_header_labels(Field_Names = "Field Name", n = "# of Records") %>% 
  colformat_num(col_keys = colkeys, digits = 0, big.mark = ",", na_str = "NA") %>% 
  set_header_labels(FieldName = "Field Name",
                    DSCRTPCategory = "DSCRTP Category") %>% 
  font(fontname = "Cambria", part = "all") %>% #header, body, all
  fontsize(size = 10, part = "body") %>% 
  autofit() 

ft


```
  
#####
  
#### Table B: Required and desired fields for different types of incoming transect-based data. 'R' means the field is required for data submission and 'D' means that it is desired by the Program. 'P' means that the field is supplied by the Program.

``` {r required_desired_transects_table, echo=FALSE}
x <-
  s %>%
  filter(InternalUseOnly == '0',
         TransHist == 'R' | TransNew == 'R' | TransProgram == 'R'
         ) %>%
  group_by(DSCRTPCategory, FieldName) %>% 
  summarize(TransHist = unique(TransHist),
            TransNew = unique(TransNew),
            TransProgram = unique(TransProgram)
                        ) %>% 
  arrange(DSCRTPCategory)

colkeys <- names(x[,-1])

ft <- flextable(x) %>% 
  theme_vanilla() %>% #booktabs, vanilla, box, tron, tron_legacy
 # set_header_labels(Field_Names = "Field Name", n = "# of Records") %>% 
  colformat_num(col_keys = colkeys, digits = 0, big.mark = ",", na_str = "NA") %>% 
  set_header_labels(FieldName = "Field Name",
                    DSCRTPCategory = "DSCRTP Category") %>% 
  font(fontname = "Cambria", part = "all") %>% #header, body, all
  fontsize(size = 10, part = "body") %>% 
  autofit() 

ft



```

##### 

#### Table C: Required and desired fields for different types of incoming trawl-based data. 'R' means the field is required for data submission and 'D' means that it is desired by the Program. 'P' means that the field is supplied by the Program.

``` {r required_desired_trawl_table, echo=FALSE}
x <-
  s %>%
  filter(InternalUseOnly == '0',
         TrawlHist == 'R' | TrawlNew == 'R' | TrawlProgram == 'R'
         ) %>%
  group_by(DSCRTPCategory, FieldName) %>% 
  summarize(TrawlHist = unique(TrawlHist),
            TrawlNew = unique(TrawlNew),
            TrawlProgram = unique(TrawlProgram)
                        ) 

colkeys <- names(x[,-1])

ft <- flextable(x) %>% 
  theme_vanilla() %>% #booktabs, vanilla, box, tron, tron_legacy
 # set_header_labels(Field_Names = "Field Name", n = "# of Records") %>% 
  colformat_num(col_keys = colkeys, digits = 0, big.mark = ",", na_str = "NA") %>% 
  set_header_labels(FieldName = "Field Name",
                    DSCRTPCategory = "DSCRTP Category") %>% 
  font(fontname = "Cambria", part = "all") %>% #header, body, all
  fontsize(size = 10, part = "body") %>% 
  autofit() 

ft




```
  
##### 
  
## Appendix 3: Data set Dashboards

Each of the data sets in the database has a unique 'DatasetID'. There are currently `r length(filt$CatalogNumber)` records in the database composed of `r length(unique(filt$DatasetID))` individual data sets from `r length(unique(filt$DataProvider))` different data providers. You will find an interactive dashboard for each data set at the following location: https://deepseacoraldata.noaa.gov/DatasetID_Table/DatasetID_Table.html These dashboards provide important information about each constituent data set that makes up the National Database and to download each set individually with ISO compliant metadata.
